---
layout: post
title: 4ì£¼ì°¨ | ë¨¸ì‹ ëŸ¬ë‹(Machine learning)(1)
description: KT AIVLE SCHOOL 5ê¸° 4ì£¼ì°¨ì— ì§„í–‰í•œ ë¨¸ì‹ ëŸ¬ë‹(Machine learning) ê°•ì˜ ë‚´ìš© ì •ë¦¬ ê¸€ì…ë‹ˆë‹¤.
sitemap: false
---

* this unordered seed list will be replaced by the toc
{:toc}

## ì „ì²˜ë¦¬(ë¦¬ë·°)

```python
def preprocess():
    # NaN ì—´ì´ í¬í•¨ëœ ëª¨ë“  ë³€ìˆ˜(axis=1) ì œê±°
    titanic.dropna(axis=1, inplace=True)

    # NaNì„ í‰ê· ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
    titanic['Age'].fillna(mean_age, inplace=True)

    # NaN ê°’ì„ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
    titanic['Embarked'].fillna(titanic['Embarked'].mode()[0], inplace=True)

    # Ozone ë³€ìˆ˜ NaN ê°’ì„ ë°”ë¡œ ì•ì˜ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
    air['Ozone'].fillna(method='ffill', inplace=True)

    # Solar.R ë³€ìˆ˜ NaN ê°’ì„ ë°”ë¡œ ë’¤ì˜ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
    air['Solar.R'].fillna(method='bfill', inplace=True)

    # ì„ í˜• ë³´ê°„ë²•ìœ¼ë¡œ ì±„ìš°ê¸°
    air['Ozone'].interpolate(method='linear', inplace=True)

    # ê°€ë³€ìˆ˜í™” - drop_first = True ë¡œ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ ì œê±° ê°€ëŠ¥
    titanic = pd.get_dummies(titanic, columns=dumm_cols, drop_first=True, dtype = int)
```


## ë¨¸ì‹ ëŸ¬ë‹ ì†Œê°œ

**(1) ë¨¸ì‹ ëŸ¬ë‹ì— ëŒ€í•œ ì´í•´**

- `ì¸ê°„ì˜ ê²½í—˜ = ë¨¸ì‹ ì˜ ë°ì´í„°`
    - **ì ë‹¹ëŸ‰ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”**
- í•™ìŠµ ë°©ë²•ì— ë”°ë¥¸ ë¶„ë¥˜
    - ì§€ë„ í•™ìŠµ : ë°ì´í„°ì˜ íŒ¨í„´ì„ ë°°ìš°ê²Œ í•˜ëŠ” í•™ìŠµ ë°©ë²•
    - ë¹„ì§€ë„ í•™ìŠµ : ì •ë‹µì´ ì—†ëŠ” ë°ì´í„° ë§Œìœ¼ë¡œ ë°°ìš°ê²Œ í•˜ëŠ” í•™ìŠµ ë°©ë²•
    - ê°•í™” í•™ìŠµ : ì„ íƒí•œ ê²°ê³¼ì— ëŒ€í•´ ë³´ìƒì„ ë°›ì•„ í–‰ë™ì„ ê°œì„ í•˜ë©´ì„œ ë°°ìš°ê²Œ í•˜ëŠ” í•™ìŠµ ë°©ë²•
- ê³¼ì œì— ë”°ë¥¸ ë¶„ë¥˜
    - ë¶„ë¥˜ ë¬¸ì œ : ì´ë¯¸ ì ì ˆíˆ ë¶„ë¥˜ëœ ë°ì´í„°ë¥¼ í†µí•´ ê·œì¹™ì„ ì°¾ì•„ ê·¸ ê·œì¹™ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡­ê²Œ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì ì ˆíˆ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ ëª©ì  **(ì§€ë„ í•™ìŠµ)**
    - íšŒê·€ ë¬¸ì œ : ì´ë¯¸ ê²°ê³¼ ê°’ì´ ìˆëŠ” ë°ì´í„°ë¥¼ í†µí•´ ì…ë ¥ê°’ê³¼ ì¶œë ¥ê°’ì˜ ì—°ê´€ì„±ì„ ì°¾ì•„ ê·¸ ì—°ê´€ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡­ê²Œ ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•œ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ëª©ì  **(ì§€ë„ í•™ìŠµ)**
    - í´ëŸ¬ìŠ¤í„°ë§ : ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ì ì ˆí•œ ë¶„ë¥˜ ê·œì¹™ì„ ì°¾ì•„ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•¨ì„ ëª©ì ìœ¼ë¡œ í•¨, ì •ë‹µì´ ì—†ìœ¼ë‹ˆ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ì–´ë ¤ì›€ **(ë¹„ì§€ë„ í•™ìŠµ)**

**(2) ë¶„ë¥˜ì™€ íšŒê·€**

- ëª¨ë¸ë§ì„ í•˜ê¸° ì „ì— ì›í•˜ëŠ” ê²°ê³¼ê°€ ë¶„ë¥˜ì¸ì§€ íšŒê·€ì¸ì§€ë¥¼ ëª…í™•íˆ ì´í•´í•´ì•¼ í•¨
- íšŒê·€ëŠ” **ì—°ì†ì ì¸ ìˆ«ì**ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ê³ , ë¶„ë¥˜ëŠ” **ë²”ì¤ ê°’**ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ
- ì—°ì†ì ì¸ ìˆ«ì í™•ì¸ ë°©ë²•
    - ë‘ ê°’ ì‚¬ì´ì— **ì¤‘ê°„ê°’**ì´ ì˜ë¯¸ê°€ ìˆëŠ” ìˆ«ìì¸ì§€
    - ë˜ëŠ” ë‘ ê°’ì— ëŒ€í•œ **ì—°ì‚° ê²°ê³¼**ê°€ ì˜ë¯¸ê°€ ìˆëŠ” ìˆ«ìì¸ì§€ ë“±
- ì˜ˆì¸¡í•´ì•¼ í•  ê°’ì— ì—°ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ë©´ ë¶„ë¥˜ì™€ íšŒê·€ë¥¼ ì‰½ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆìŒ
    - ë¶„ë¥˜: Aì¼ê¹Œ? Bì¼ê¹Œ?, íšŒê·€: ì–¼ë§ˆë‚˜ ë§ì´?
- **ë¶„ë¥˜ì™€ íšŒê·€ëŠ” ì„œë¡œ ë‹¤ë¥¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ë§ì„ í•˜ê²Œ ë¨ (ì¤‘ìš”)**
    - **ë¬¸ì œ ìœ í˜•ì„ ì •í™•íˆ íŒŒì•… â†’ ì•Œê³ ë¦¬ì¦˜ê³¼ í‰ê°€ ë°©ë²•ì„ ì„ íƒ â†’ ê´€ë ¨ëœ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ë§**
    

**(3) ë¯¸ë¦¬ ì•Œì•„ë‘˜ ìš©ì–´**

- ëª¨ë¸, ëª¨ë¸ë§
    - ëª¨ë¸ : ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ ì°¾ì•„ ìˆ˜ì‹ìœ¼ë¡œ ì •ë¦¬í•´ ë†“ì€ ê²ƒ
    - ëª¨ë¸ë§ : ì˜¤ì°¨ê°€ ì ê³  ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“œëŠ” ê³¼ì •
    - ëª¨ë¸ì˜ ëª©ì 
        - ìƒ˜í”Œì„ ê°€ì§€ê³  ì „ì²´ë¥¼ ì¶”ì •
- í–‰, ì—´
    - í–‰ : ê°œì²´, **ê´€ì¸¡ì¹˜**, ê¸°ë¡, ì‚¬ë¡€, ê²½ìš°
    - ì—´ : íŠ¹ì„±, ì†ì„±, **ë³€ìˆ˜**, í•„ë“œ
- ë…ë¦½ë³€ìˆ˜, ì¢…ì†ë³€ìˆ˜
    - ì›ì¸ê³¼ ê²°ê³¼ (x, y)
- ì˜¤ì°¨
    - í‰ê· ê³¼ ì˜¤ì°¨
        - í†µê³„í•™ì—ì„œ ì‚¬ìš©ë˜ëŠ” **ê°€ì¥ ë‹¨ìˆœí•œ ëª¨ë¸** ì¤‘ í•˜ë‚˜ : **í‰ê· **
        - ê´€ì¸¡ê°’(=ì‹¤ì ¯ê°’)ê³¼ ëª¨ë¸ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´: ì´íƒˆë„(Deviance) â†’ **ì˜¤ì°¨**
- ë°ì´í„° ë¶„ë¦¬
    - ì‹¤ì „ : í•™ìŠµìš©, ê²€ì¦ìš©, í‰ê°€ìš© ë°ì´í„°ë¡œ ë¶„ë¦¬
        - í‰ê°€ìš© : ë³„ë„ ì œê³µ ë°ì´í„°ì¼ ê²½ìš° ë§ìŒ
        - ê²€ì¦ìš© : í‰ê°€ ì „ì— ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦ ê°€ëŠ¥ (íŠœë‹ ì‹œ ì‚¬ìš©)
    - ìˆ˜ì—… : í¸ì˜ìƒ ëª¨ë¸ ìƒì„± í›„ í‰ê°€ìš©ìœ¼ë¡œ ë°”ë¡œ í‰ê°€
- ê³¼ëŒ€ì í•© vs ê³¼ì†Œì í•©
    - ê³¼ëŒ€ì í•©
        - í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì€ë°, í‰ê°€ ë°ì´í„°ì—ì„œ ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì§€ ì•Šì€ ê²½ìš°
        - í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œ ë§Œ ì˜ ë§ëŠ” ëª¨ë¸ â†’ **ì‹¤ì „ì—ì„œ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•ŠìŒ**
    - ê³¼ì†Œì í•©
        - í•™ìŠµ ë°ì´í„°ë³´ë‹¤ í‰ê°€ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ê±°ë‚˜, ëª¨ë“  ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì´ ë§¤ìš° ì•ˆ ì¢‹ì€ ê²½ìš°
        - **ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœ**í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ ì ì ˆíˆ í›ˆë ¨ë˜ì§€ ì•Šì€ ê²½ìš°

**(4) ë°ì´í„° ì¤€ë¹„ ê³¼ì •**

- ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ ê·œì¹™ì„ ì°¾ê³  ê·¸ ê·œì¹™ì— ê¸°ë°˜í•´ ì˜ˆì¸¡ í•˜ëŠ” ê²ƒ â†’ ë°ì´í„° ì¤€ë¹„ ê³¼ì •ì´ í•„ìš”!!!
- ì¶©ë¶„íˆ í•™ìŠµí•˜ê³  í‰ê°€í•  ìˆ˜ ìˆëŠ” ì¢‹ì€ ë°ì´í„°ê°€ ì¤€ë¹„ ë˜ì–´ì•¼ ì¢‹ì€ ëª¨ë¸ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŒ
- ëŒ€ìƒ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ì¶©ë¶„íˆ íƒìƒ‰í•˜ê³  ì ì ˆíˆ ì „ì²˜ë¦¬ í•œ í›„ ë°ì´í„° ë¶„ë¦¬ ê³¼ì •ì„ ìˆ˜í–‰
    
**(5) ì‹¤ìŠµ**

```python
# ìƒê´€ê´€ê³„ ì‹œê°í™”
sns.heatmap(data.corr(), annot = True, cmap = 'Blues', cbar = False,
            square = True, fmt = '.2f', annot_kws = {'size' : 9})
plt.show()

# ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì ¯ê°’ ì‹œê°í™” ë¹„êµ
medv_mean = y_train.mean()
print(f'í‰ê·  : {medv_mean}')

plt.plot(y_test.values, label = 'Actual')
plt.plot(y_pred, label = 'Predicted')
plt.legend()
plt.axhline(medv_mean, color = 'r')
plt.show()

# GPA --> ADMIT (ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì— ë”°ë¥¸ ë²”ì£¼í˜• ë³€ìˆ˜(target)ì˜ ë¶„í¬ í™•ì¸)
sns.histplot(x = data['GPA'], hue = data['ADMIT'], bins = 30)
plt.show()

# 7:3ìœ¼ë¡œ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,
                                                    random_state = 1, shuffle = True, stratify = y) 
# ì‹œê³„ì—´ ë°ì´í„°ì—ì„œëŠ” shuffle ê¸ˆì§€(default: True), stratify : ë¼ë²¨ ë³„ë¡œ ê· ë“±í•˜ê²Œ ë¶„í¬ë˜ë„ë¡ í•´ì£¼ëŠ” ì˜µì…˜
```

## ì„±ëŠ¥ í‰ê°€

### íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

> ì˜ˆì¸¡ ê°’ì´ ì‹¤ì œ ê°’ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ â†’ ì˜¤ì°¨ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

**(1) ìš©ì–´ ì •ë¦¬**

> $$\Large y$$ : ì‹¤ì œê°’ â†’ ì‹¤ì œ ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ê°’, ì˜¤ì°¨ : ì´ ê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´\
> $$\Large \bar{y}$$ : í‰ê· ê°’ â†’ ì´ë¯¸ ì¡´í•´í•˜ëŠ” í‰ê· ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê°’\
> $$\Large \hat{y}$$ : ì˜ˆì¸¡ê°’ â†’ ìƒˆë¡­ê²Œ ëª¨ë¸ë¡œ ì˜ˆì¸¡í•œ ê°’, í‰ê· ê°’ë³´ë‹¤ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í–ˆëŠ”ì§€ í™•ì¸

**(2) íšŒê·€ í‰ê°€ ì§€í‘œ ì •ë¦¬**

> $$\Large MSE = \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{n}$$\
> $$\Large RMSE = \sqrt{MSE} = \sqrt{\frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{n}}$$\
> $$\Large MAE = \frac{\sum_{i=1}^{n} |y_i - \hat{y}_i|}{n}$$\
> $$\Large MAPE = \frac{\sum_{i=1}^{n} \left|\frac{y_i - \hat{y}_i}{y_i}\right|}{n}$$
> > **ìœ„ ê°’ ëª¨ë‘ ì‘ì„ ìˆ˜ë¡ ëª¨ë¸ ì„±ëŠ¥ì´ ì¢‹ë‹¤**

**(3) ì˜¤ì°¨ë¥¼ ë³´ëŠ” ë‹¤ì–‘í•œ ê´€ì **

> $$\Large SST = \sum_{i=1}^{n} (y_i - \bar{y})^2$$ : **ì „ì²´ ì˜¤ì°¨**\
> $$\Large SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$ : **íšŒê·€ì‹ì´ ì¡ì•„ë‚´ì§€ ëª»í•œ ì˜¤ì°¨**\
> $$\Large SSR = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2$$ : **íšŒê·€ì‹ì´ ì¡ì•„ë‚¸ ì˜¤ì°¨**\
> $$\Large SST = SSR + SSE$$

**(4) ê²°ì • ê³„ìˆ˜ (R-Squared)**
- **ì „ì²´ ì˜¤ì°¨ ì¤‘ì—ì„œ íšŒê·€ì‹ì´ ì¡ì•„ë‚¸ ì˜¤ì°¨ ë¹„ìœ¨**
- ëª¨ë¸ì˜ **ì„¤ëª…ë ¥**ì´ë¼ê³ ë„ ë¶€ë¦„ (ê°’ì´ í´ìˆ˜ë¡ ì¢‹ìŒ)

> $$\Large R^{2} = \frac{SSR}{SST} = 1 -  \frac{SSE}{SST} = 1-\frac{\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^2}{\sum_{i=1}^{n}(y_{i}-\bar{y}_{i})^2}$$

**(5) ì‹¤ìŠµ**
    
```python
# 1. ë°ì´í„° ì¤€ë¹„
target = 'medv' ## target í™•ì¸
X = data.drop(target, axis = 1) ## ë°ì´í„° ë¶„ë¦¬
y = data.loc[:, target]
## 7:3ìœ¼ë¡œ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)

# 2. ëª¨ë¸ë§
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 3. íšŒê·€ ì„±ëŠ¥ í‰ê°€
print(f'MAE : {mean_absolute_error(y_test, y_pred)}')
print(f'MSE : {mean_squared_error(y_test, y_pred)}')
print(f'RMSE : {mean_squared_error(y_test, y_pred, squared = False)}')
print(f'MAPE : {mean_absolute_percentage_error(y_test, y_pred)}')
print(f'R2 : {r2_score(y_test, y_pred)}')
```
    

### ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

>- ë¶„ë¥˜ ëª¨ë¸ì€ 0ì¸ì§€ 1ì¸ì§€ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ
>- ì˜ˆì¸¡ ê°’ì´ ì‹¤ì œ ê°’ê³¼ ì¼ì¹˜í•˜ëŠ” ê°’ì´ ë§ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ â†’ ì •í™•íˆ ì˜ˆì¸¡í•œ ë¹„ìœ¨ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

**(1) Confusion Matrix(í˜¼ë™ í–‰ë ¬)**
    
|  | ì˜ˆì¸¡Negative(0) | ì˜ˆì¸¡Positive(1) |
| --- | --- | --- |
| ì‹¤ì œNegative(0) | $$TN$$ | $$FP$$ |
| ì‹¤ì œPositive(1) | $$FN$$ | $$TP$$ |

- ìš©ì–´ : **ê²°ê³¼(T / F) + ì˜ˆì¸¡ (P / N), í–‰ë ¬ì˜ ì—­ìŠ¬ë˜ì‹œ ê°’ì€ í•­ìƒ ì •ë‹µ**
- ì •í™•ë„(Accuracy) = $$\Large \frac{TN + TP}{TN + FP + FN + TP}$$
    - ì •ë¶„ë¥˜ìœ¨
    - ê°€ì¥ ì§ê´€ì ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ í™•ì¸ ê°€ëŠ¥í•œ í‰ê°€ì§€í‘œ
- ì •ë°€ë„(Precision) = $$\Large \frac{TP}{FP + TP}$$
    - ì˜ˆì¸¡ ê´€ì 
    - ì •ë°€ë„ê°€ ë‚®ì„ ê²½ìš° ë°œìƒí•˜ëŠ” ë¬¸ì œ
        - ì•”ì´ ì•„ë‹Œë° ì•”ì´ë¼ í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì¹˜ë£Œ ë°œìƒ
- ì¬í˜„ìœ¨(Recall) = $$\Large \frac{TP}{FN + TP}$$
    - ì‹¤ì œ ê´€ì 
    - ë¯¼ê°ë„(Sensitivity)ë¼ê³ ë„ ë¶€ë¦„
    - ì¬í˜„ìœ¨ì´ ë‚®ì„ ê²½ìš° ë°œìƒí•˜ëŠ” ë¬¸ì œ
        - ì•”ì¸ ì‚¬ëŒì—ê²Œ ì•”ì´ ì•„ë‹ˆë¼ê³  í•˜ëŠ” ê²½ìš°
- ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì€ ê¸°ë³¸ì ìœ¼ë¡œ **Positive**ì— ëŒ€í•´ì„œ ì´ì•¼ê¸°
    - **Negative**ì— ëŒ€í•œ ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ë„ ì˜ë¯¸ë¥¼ ê°€ì§
- íŠ¹ì´ë„(Specificity) = $$\Large \frac{TN}{FP + TN}$$
    - ì‹¤ì œ Negative ì¤‘ì—ì„œ Negativeë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨
    - íŠ¹ì´ë„ê°€ ë‚®ì„ ê²½ìš° ë°œìƒí•˜ëŠ” ë¬¸ì œ
        - ì•”ì´ ì•„ë‹Œë° ì•”ì´ë¼ í–ˆìœ¼ë‹ˆ ë¶ˆí•„ìš”í•œ ì¹˜ë£Œê°€ ë°œìƒ

**(2) F1-Score**
- ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· 
- ê´€ì ì´ ë‹¤ë¥¸ ê²½ìš° ì¡°í™”í‰ê· ì´ í° ì˜ë¯¸ë¥¼ ê°€ì§
- ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì´ ì ì ˆí•˜ê²Œ ìš”êµ¬ë  ë•Œ ì‚¬ìš©
- F1-Score = $$\Large \frac{2 \times Precision \times Recall}{Precision + Recall}$$

**(3) ì‹¤ìŠµ**
    
```python
# 1. ë°ì´í„° ì¤€ë¹„
target = 'ADMIT' # target í™•ì¸
x = data.drop(target, axis=1) # ë°ì´í„° ë¶„ë¦¬
y = data.loc[:, target]

# 2. ëª¨ë¸ë§
model = KNeighborsClassifier()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

# 3.ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€

print('Confusion Matrix\n', confusion_matrix(y_test, y_pred), '\n') # ì„±ëŠ¥ í‰ê°€

## í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
plt.figure(figsize = (5, 3))
sns.heatmap(confusion_matrix(y_test, y_pred), annot = True, cmap = 'Blues', cbar = False)
plt.show()

print(f'Accuracy : {accuracy_score(y_test, y_pred)}')
## ì°¸ê³  : ë‘ ìˆ˜ì˜ ì°¨ì´ë¡œ ê³¼ëŒ€ì í•©, ê³¼ì†Œì í•©ì„ ì•Œ ìˆ˜ ìˆìŒ (ì•½ê°„ ê³¼ëŒ€ì í•©)
print(f'í‰ê°€ ì„±ëŠ¥(ì •í™•ë„) : {model.score(x_test, y_test)}') 
print(f'í•™ìŠµ ì„±ëŠ¥(ì •í™•ë„) : {model.score(x_train, y_train)}\n')

print('Precision : ', precision_score(y_test, y_pred)) # default : 1ì— ëŒ€í•œ ì •ë°€ë„
print('Precision : ', precision_score(y_test, y_pred, average = 'binary')) # default
print('Precision : ', precision_score(y_test, y_pred, average = None)) # ë‘˜ë‹¤ ì¶œë ¥
print('Precision : ', precision_score(y_test, y_pred, average = 'macro')) # í‰ê· 
print('Precision : ', precision_score(y_test, y_pred, average = 'weighted'), '\n') # ê°€ì¤‘ì¹˜

print('Recall : ', recall_score(y_test, y_pred))
print('Recall : ', recall_score(y_test, y_pred, average = None), '\n') # ë‘˜ë‹¤ ì¶œë ¥ ê¸°ì–µí•˜ê¸°

print('F1-Score : ', f1_score(y_test, y_pred))
print('F1-Score : ', f1_score(y_test, y_pred, average = None), '\n') # ë‘˜ë‹¤ ì¶œë ¥ ê¸°ì–µí•˜ê¸°

print('Classification_report \n\n', classification_report(y_test, y_pred))
```

## ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜

### Linear Regression

**(1) ì •ì˜ ë° íŠ¹ì§•**
- ìµœì„ ì˜ íšŒê·€ ëª¨ë¸ : **ì˜¤ì°¨ í•©ì´ ìµœì†Œ**ê°€ ë˜ëŠ” ëª¨ë¸(ì˜¤ì°¨ í•©ì´ ìµœì†Œê°€ ë˜ëŠ” ê°€ì¤‘ì¹˜, í¸í–¥ì„ ì°¾ê¸°)
- ë‹¨ìˆœ íšŒê·€ : ë…ë¦½ ë³€ìˆ˜ í•˜ë‚˜ê°€ ì¢…ì† ë³€ìˆ˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì„ í˜• íšŒê·€
    
    ```python
    # íšŒê·€ê³„ìˆ˜ í™•ì¸
    print(model.coef_)
    print(model.intercept_)
    ```
    
- ë‹¤ì¤‘ íšŒê·€ : ì—¬ëŸ¬ ë…ë¦½ ë³€ìˆ˜ê°€ ì¢…ì† ë³€ìˆ˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì„ í˜• íšŒê·€
    
    ```python
    # íšŒê·€ê³„ìˆ˜ í™•ì¸
    print(list(x_train))
    print(model.coef_)
    print(model.intercept_)
    ```
    
**(2) ì‹¤ìŠµ**
    
```python
# 1. ë°ì´í„° ì¤€ë¹„
target = 'dist' # target í™•ì¸
x = data.drop(target, axis=1) # ë°ì´í„° ë¶„ë¦¬
y = data.loc[:, target]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)

# 2. ëª¨ë¸ë§
model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
print('MAE : ', mean_absolute_error(y_test, y_pred))
print('R2 : ', r2_score(y_test, y_pred))
# íšŒê·€ê³„ìˆ˜ í™•ì¸
print(model.coef_)
print(model.intercept_)

# 3. ê¸°íƒ€
# íšŒê·€ì‹ ë§Œë“¤ê¸°
a = model.coef_
b = model.intercept_
speed = np.linspace(x_test.min(), x_test.max(), 10)
dist = a * speed + b

# íšŒê·€ì„  í‘œì‹œ
dist_mean = y_train.mean()
plt.scatter(x_test, y_test) # í‰ê°€ ë°ì´í„°
plt.scatter(x_test, y_pred) # í•™ìŠµ ë°ì´í„°
plt.plot(speed, dist, color = 'r') # í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’
plt.axhline(dist_mean, color = 'r', linestyle = '--')
plt.title('Speed & Distance', size = 20, pad = 10)
plt.xlabel('Speed(mph)')
plt.ylabel('Dist(ft)')
plt.show()

# ì‹œê°í™”
plt.plot(y_test.values, label='Actual')
plt.plot(y_pred, label='Predicted')
plt.legend()
plt.ylabel('Dist(ft)')
plt.show()
```
    
### K-Nearest Neighbor

**(1) ì •ì˜ ë° íŠ¹ì§•**
- kê°œì˜ ìµœê·¼ì ‘ ì´ì›ƒì˜ ê°’ì„ ì°¾ì•„ ê·¸ ê°’ë“¤ë¡œ ìƒˆë¡œìš´ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
- íšŒê·€ì™€ ë¶„ë¥˜ì— ì‚¬ìš©ë˜ëŠ” ë§¤ìš° ê°„ë‹¨í•œ ì§€ë„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ â†’ ì—°ì‚° ì†ë„ëŠ” ëŠë¦¼
    - kê°œ ê°’ì˜ í‰ê· ìœ¼ë¡œ ì˜ˆì¸¡, ê°€ì¥ ë§ì´ í¬í•¨ëœ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜
- kê°’ì˜ ì¤‘ìš”ì„±
    - **ì ì ˆí•œ kê°’ì„ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”(ê¸°ë³¸ê°’ = 5)**
    - **ì¼ë°˜ì ìœ¼ë¡œ 1ì´ ì•„ë‹Œ í™€ìˆ˜ë¡œ ì„¤ì •**
- ê±°ë¦¬ êµ¬í•˜ê¸°
    - ë§¨í•˜íŠ¼ ê±°ë¦¬(ë‘ ì§€ì ì˜ ê° ì¢Œí‘œì˜ ì°¨ì˜ ì ˆëŒ€ê°’) â‰¥ ìœ í´ë¦¬ë“œ ê±°ë¦¬(ë‘ ì§€ì ì˜ ê±°ë¦¬)
- Scaling í•„ìš”ì„±
    - ìŠ¤ì¼€ì¼ë§ ì—¬ë¶€ì— ë”°ë¼ KNN ëª¨ë¸ ì„±ëŠ¥ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
    - ëŒ€í‘œì ì¸ ìŠ¤ì¼€ì¼ë§: **ì •ê·œí™”(Normalization), í‘œì¤€í™”(Standardization)**
    - í‰ê°€ìš© ë°ì´í„°ì—ë„ **í•™ìŠµìš© ë°ì´í„°** **ê¸°ì¤€**ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ ìˆ˜í–‰
        
        **[ì°¸ê³ ] í•™ìŠµ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”**
        
        <img src = 'https://github.com/Jangrae/img/blob/master/minmax.png?raw=true'>
        
**(2) ì‹¤ìŠµ**
    
```python
# 1. ë°ì´í„° ì¤€ë¹„
data.interpolate(method='linear', inplace=True) # ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°

#  ë³€ìˆ˜ ì œê±°
drop_cols = ['Month', 'Day'] 
data.drop(drop_cols, axis=1, inplace=True)

target = 'Ozone' # target í™•ì¸

# ë°ì´í„° ë¶„ë¦¬
x = data.drop(target, axis=1)
y = data.loc[:, target]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)

# ì •ê·œí™”
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# 2. ëª¨ë¸ë§
model = KNeighborsRegressor() # n_neighborsë¥¼ ë°ì´í„°ì˜ ìˆ˜ë¡œ í•˜ë©´ ëª¨ë¸ì´ í‰ê· ê°’ì´ ëœë‹¤
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

print('MAE : ', mean_absolute_error(y_test, y_pred))
print('R2 : ', r2_score(y_test, y_pred))

# 3. ê¸°íƒ€
# ì˜ˆì¸¡ê°’, ì‹¤ì ¯ê°’ ì‹œê°í™”
plt.plot(y_test.values, label='Actual')
plt.plot(y_pred, label='Predicted')
plt.legend()
plt.ylabel('Ozone')
plt.show()
```
    

### Decision Tree

**(1) ì •ì˜ ë° íŠ¹ì§•**
- íŠ¹ì • ë³€ìˆ˜ì— ëŒ€í•œ ì˜ì‚¬ê²°ì • ê·œì¹™ì„ **ë‚˜ë¬´ ê°€ì§€**ê°€ ë»—ëŠ” í˜•íƒœë¡œ ë¶„ë¥˜í•´ ë‚˜ê°
- **ë¶„ë¥˜**ì™€ **íšŒê·€** ëª¨ë‘ì— ì‚¬ìš©ë˜ëŠ” ì§€ë„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜
- ë¶„ì„ ê³¼ì •ì„ ì‹¤ì œë¡œ í™•ì¸ ê°€ëŠ¥ â†’ **í™”ì´íŠ¸ ë°•ìŠ¤ ëª¨ë¸**
- **ì˜ë¯¸ ìˆëŠ” ì§ˆë¬¸**ì„ ë¨¼ì € í•˜ëŠ” ê²ƒì´ ì¤‘ìš”
- **ê³¼ì í•©** ë°œìƒí•˜ê¸° ì‰¬ì›€
    - **íŠ¸ë¦¬ ê¹Šì´ë¥¼ ì œí•œ**í•˜ëŠ” íŠœë‹ì´ í•„ìš”
- Root Node(ë¿Œë¦¬ ë§ˆë””), Terminal Node(ë ë§ˆë””, Leaf Node), Depth(ê¹Šì´)
- ë¶„ë¥˜ì™€ íšŒê·€
    - ë¹„ìš©í•¨ìˆ˜: ë¶„ë¥˜ â†’ **ë¶ˆìˆœë„**, íšŒê·€ â†’ **MSE**
    - ë¶„ë¥˜: ë§ˆì§€ë§‰ ë…¸ë“œì— ìˆëŠ” ìƒ˜í”Œë“¤ì˜ **ìµœë¹ˆê°’**ì„ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ë°˜í™˜
    - íšŒê·€: ë§ˆì§€ë§‰ ë…¸ë“œì— ìˆëŠ” ìƒ˜í”Œë“¤ì˜ **í‰ê· **ì„ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ë°˜í™˜

**(2) ë¶ˆìˆœë„(Impurity)**
- ë¶ˆìˆœë„ê°€ ë‚®ì„ ìˆ˜ë¡ ë¶„ë¥˜ê°€ ì˜ ëœ ê²ƒ
- ë¶ˆìˆœë„ ìˆ˜ì¹˜í™” ê°€ëŠ¥í•œ ì§€í‘œ
    - ì§€ë‹ˆ ë¶ˆìˆœë„(Gini Impurity)
        > $$- (ì–‘ì„± í´ë˜ìŠ¤ ë¹„ìœ¨^2 + ìŒì„± í´ë˜ìŠ¤ ë¹„ìœ¨^2)$$
        
        - ë¶„ë¥˜ í›„ ì–¼ë§ˆë‚˜ ì˜ ë¶„ë¥˜í–ˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ì§€í‘œ
        - íŠ¹ì§•
            - ì§€ë‹ˆ ë¶ˆìˆœë„ê°€ ë‚®ì„ìˆ˜ë¡ ìˆœë„ê°€ ë†’ìŒ
            - 0 ~ 0.5 ì‚¬ì´ì˜ ê°’(ì´ì§„ ë¶„ë¥˜ì˜ ê²½ìš°) â†’ ìˆœìˆ˜í•˜ê²Œ ë¶„ë¥˜: 0, ì™„ë²½í•˜ê²Œ ì„ì´ë©´: 0.5
        - ì§€ë‹ˆ ë¶ˆìˆœë„ê°€ ë‚®ì€ ì†ì„±ìœ¼ë¡œ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ ë…¸ë“œ ê²°ì •
    - ì—”íŠ¸ë¡œí”¼(Entropy)
        > $$- ìŒì„±í´ë˜ìŠ¤ë¹„ìœ¨ \times log_2(ìŒì„± í´ë˜ìŠ¤ ë¹„ìœ¨) - ì–‘ì„±í´ë˜ìŠ¤ë¹„ìœ¨ \times log_2(ì–‘ì„± í´ë˜ìŠ¤ ë¹„ìœ¨)$$
        
        - $$p_i$$ : ì§‘í•© ì•ˆì—ì„œ ì†ì„± iì˜ í™•ë¥ 
        - 0 ~ 1ì‚¬ì´ì˜ ê°’ â†’ ìˆœìˆ˜í•˜ê²Œ ë¶„ë¥˜ë˜ë©´: 0, ì™„ë²½í•˜ê²Œ ì„ì´ë©´: 1
        - ì •ë³´ ì´ë“(Information Gain)
            > $$ğºğ‘ğ‘–ğ‘› (ğ‘‡, ğ‘‹) = ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ (ğ‘‡) âˆ’ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(ğ‘‡, ğ‘‹)$$
            
            - ì •ë³´ ì´ë“ì´ í¬ë‹¤ = ì–´ë–¤ ì†ì„±ìœ¼ë¡œ ë¶„í• í•  ë•Œ ë¶ˆìˆœë„ê°€ ì¤„ì–´ë“ ë‹¤
            - ì •ë³´ ì´ë“ì´ ê°€ì¥ í° ì†ì„±ë¶€í„° ë¶„í• 

**(3) ê°€ì§€ì¹˜ê¸°**
- ê°€ì§€ì¹˜ê¸°ë¥¼ í•˜ì§€ ì•Šìœ¼ë©´ â†’ ê³¼ëŒ€ì í•©, ì¼ë°˜í™”ë˜ì§€ ëª»í•¨
- ì—¬ëŸ¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ì¡°ì •í•´ ê°€ì§€ì¹˜ê¸° í•  ìˆ˜ ìˆìŒ
    - **max_depth(íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´(ê¸°ë³¸ê°’: None))**
    - **min_samples_leaf(ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ê°œìˆ˜(ê¸°ë³¸ê°’: 2))**
    - **min_samples_split(ë¦¬í”„ ë…¸ë“œê°€ ë˜ê¸° ìœ„í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ìˆ˜(ê¸°ë³¸ê°’: 1))**
- ê°€ì¥ ì ì ˆí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ ì°¾ë„ë¡ ë…¸ë ¥í•´ì•¼ í•¨
- Decision Tree ë¶„ë¥˜ ëª¨ë¸ë„ ê²°êµ­ **í™•ë¥ **ì— ê·¼ê±°í•´ **ì˜ˆì¸¡**ì„ í•œë‹¤

**(4) ì‹¤ìŠµ**
    
```python
# 1. ë°ì´í„° ì¤€ë¹„
# ì œê±° ëŒ€ìƒ: PassengerId, Name, Ticket, Cabin
drop_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']
# ë³€ìˆ˜ ì œê±°
data.drop(drop_cols, axis=1, inplace=True)
# Age ê²°ì¸¡ì¹˜ë¥¼ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
age_median = data['Age'].median()
data['Age'].fillna(age_median, inplace=True)
# Embarked ìµœë¹ˆê°’ìœ¼ë¡œ ì±„ìš°ê¸°
emb_freq = data['Embarked'].mode()[0]
data['Embarked'].fillna(emb_freq, inplace=True)

# target í™•ì¸
target = 'Survived'
# ë°ì´í„° ë¶„ë¦¬
x = data.drop(target, axis=1)
y = data.loc[:, target]
# ê°€ë³€ìˆ˜í™” ëŒ€ìƒ: Pclass, Sex, Embarked
dumm_cols = ['Pclass', 'Sex', 'Embarked']
# ê°€ë³€ìˆ˜í™”
x = pd.get_dummies(x, columns=dumm_cols, drop_first=True, dtype=int)
# 7:3ìœ¼ë¡œ ë¶„ë¦¬
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)

# 2. ëª¨ë¸ë§
model = DecisionTreeClassifier(max_depth = 5, random_state=1)
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

print(confusion_matrix(y_test, y_pred), '\n')
print(classification_report(y_test, y_pred))

# 3. ê¸°íƒ€
# ì´ë¯¸ì§€ íŒŒì¼ ë§Œë“¤ê¸°
export_graphviz(model,                                 # ëª¨ë¸ ì´ë¦„
                out_file='tree.dot',                   # íŒŒì¼ ì´ë¦„
                feature_names=x.columns,               # Feature ì´ë¦„
                class_names=['die', 'survived'],       # Target Class ì´ë¦„
                rounded=True,                          # ë‘¥ê·¼ í…Œë‘ë¦¬
                precision=2,                           # ë¶ˆìˆœë„ ì†Œìˆ«ì  ìë¦¬ìˆ˜
                # max_depth = 3,                         # í‘œì‹œí•  íŠ¸ë¦¬ ê¹Šì´
                filled=True)                           # ë°•ìŠ¤ ë‚´ë¶€ ì±„ìš°ê¸°

# íŒŒì¼ ë³€í™˜
!dot tree.dot -Tpng -otree.png -Gdpi=300
# ì´ë¯¸ì§€ íŒŒì¼ í‘œì‹œ
Image(filename='tree.png')

# ë³€ìˆ˜ ì¤‘ìš”ë„ ë°ì´í„°í”„ë ˆì„ ë§Œë“¤ê¸°
df = pd.DataFrame()
df['feature'], df['importance'] = list(x), model.feature_importances_
df.sort_values(by='importance', ascending=True, inplace=True)
# ì‹œê°í™”
plt.figure(figsize=(5, 5))
plt.barh(df['feature'], df['importance']) # ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•´ì•¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ë³´ì—¬ì§„ë‹¤
plt.show()
```

### Logistic Regression

**(1) ì •ì˜ì™€ íŠ¹ì§•**
- ë¡œì§€ìŠ¤í‹± íšŒê·€: í™•ë¥  ë¬¸ì œë¥¼ ì„ í˜•íšŒê·€ë¡œ ëª¨ë¸ë§
- ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜
    - **ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜** : $$\Large p = \frac {1} {1 + e^{-f(x)}}$$
    - (-âˆ, âˆ) ë²”ìœ„ë¥¼ ê°–ëŠ” ì„ í˜• íŒë³„ì‹ ê²°ê³¼ë¡œ (0, 1) ë²”ìœ„ì˜ í™•ë¥  ê°’ì„ ì–»ê²Œ ë¨
    - **í•™ìŠµ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” ì„ í˜• íŒë³„ì‹ì˜ ê¸°ìš¸ê¸°(ğ‘)ì™€ ì ˆí¸(ğ‘)ì„ ì°¾ëŠ” ë¬¸ì œ**

**(2) ì‹¤ìŠµ**
    
```python
# 1.ë°ì´í„° ì¤€ë¹„
target = 'Outcome' # target í™•ì¸
# ë°ì´í„° ë¶„ë¦¬
x = data.drop(target, axis=1)
y = data.loc[:, target]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)

# 2. ëª¨ë¸ë§
model = LogisticRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# 3. ê¸°íƒ€
# ì˜ˆì¸¡ê°’ í™•ì¸
print(y_test.values[10:30])
print(y_pred[10:30])

# í™•ë¥ ê°’ í™•ì¸
p = model.predict_proba(x_test)
print(p[10:30])

# 1ì˜ í™•ë¥ ê°’ ì–»ê¸°
p1 = p[:, [1]]
# ì„ê³„ê°’ 0.5
y_pred2 = np.array([1 if x > 0.5 else 0 for x in p1])
print(y_pred2[:20])
print(classification_report(y_test, y_pred2))
# ì„ê³„ê°’ 0.45
y_pred2 = np.array([1 if x > 0.45 else 0 for x in p1])
print(y_pred2[:20])
print(classification_report(y_test, y_pred2))
```