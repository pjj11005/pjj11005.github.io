---
layout: post
title: 6ì£¼ì°¨ | ë”¥ëŸ¬ë‹(Deep learning)(2)
description: KT AIVLE SCHOOL 5ê¸° 6ì£¼ì°¨ì— ì§„í–‰í•œ ë”¥ëŸ¬ë‹(Deep learning) ê°•ì˜ ë‚´ìš© ì •ë¦¬ ê¸€ì…ë‹ˆë‹¤.
sitemap: false
---

* this unordered seed list will be replaced by the toc
{:toc}

## ì„±ëŠ¥ê´€ë¦¬

### ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ ë†’ì´ê¸°

- ë°ì´í„°
    - ì…ë ¥ ë°ì´í„° ì •ì œ, ì ì ˆí•œ ì „ì²˜ë¦¬
    - ë°ì´í„° ëŠ˜ë¦¬ê¸°:
        - ì—´(ì ì ˆí•œ feature ì¶”ê°€) â†’ ì„±ëŠ¥ í–¥ìƒ (Bias ì¤„ì´ê¸°)
        - í–‰(ë°ì´í„° ê±´ìˆ˜ ëŠ˜ë¦¬ê¸°) â†’ í¸ì°¨ ì¤„ì´ê¸°(Variance ì¤„ì´ê¸°)
- ëª¨ë¸ êµ¬ì¡°
    - Hidden Layer, ë…¸ë“œ ìˆ˜ ëŠ˜ë¦¬ê¸° : ì„±ëŠ¥ì´ ì¦ê°€í•  ë•Œ ê¹Œì§€
    - ë°˜ë³µë¬¸ / keras-tuner
- í•™ìŠµ
    - epochs : 10 ~ 50ì—ì„œ ì‹œì‘
        - Model check point / Early Stopping ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì €ì¥ ê°€ëŠ¥
    - learning rate : 0.1 ~ 0.001 ì‚¬ì´ì—ì„œ ì‹œì‘

### ê³¼ì í•© ë¬¸ì œ

- ëª¨ë¸ë§ ëª©ì  : ëª¨ì§‘ë‹¨ ì „ì²´ì—ì„œ ë‘ë£¨ ì˜ ë§ì¶”ëŠ” (ì ë‹¹í•œ) ëª¨ë¸ ë§Œë“¤ê¸°
- ê³¼ì í•© : í•™ìŠµ ë°ì´í„°ì—ì„œë§Œ ë†’ì€ ì„±ëŠ¥, ë‹¤ë¥¸ ë°ì´í„°ì—ì„œëŠ” ë‚®ì€ ì„±ëŠ¥

### ê³¼ì í•© ë°©ì§€í•˜ê¸°

1. ì ì ˆí•œ ëª¨ë¸ ìƒì„±
    - ëª¨ë¸ì˜ ë³µì¡ë„ : í•™ìŠµìš© ë°ì´í„°ì˜ íŒ¨í„´ì„ ë°˜ì˜í•˜ëŠ” ì •ë„
    - ì ì ˆí•œ ë³µì¡ë„ ì§€ì  ì°¾ê¸° : ë³µì¡ë„ë¥¼ ì¡°ê¸ˆì”© ì¡°ì ˆí•´ ê°€ë©´ì„œ Train errorì™€ Validation errorë¥¼ ì¸¡ì •í•˜ê³  ë¹„êµ
    - ë”¥ëŸ¬ë‹ì—ì„œì˜ ì¡°ì ˆ
        - Epochì™€ learning_rate
        - ëª¨ë¸ êµ¬ì¡° ì¡°ì • : hidden layer, node ìˆ˜
        - Early Stopping
        - Regularization(ê·œì œ) : L1, L2
        - Dropout
2. Early Stopping
    - epochê°€ ë§ìœ¼ë©´ ê³¼ì í•© ë  ìˆ˜ ìˆìŒ
        - í•­ìƒ ë°œìƒí•˜ì§€ëŠ” ì•Šì§€ë§Œ ë°˜ë³µí• ìˆ˜ë¡ ì˜¤ì°¨ ê°ì†Œ â†’ ì¦ê°€í•  ìˆ˜ ë„ ìˆìŒ
    - ì˜µì…˜
        - `monitor` : ê¸°ë³¸ê°’ (`val_loss`)
        - `min_delta` : ì˜¤ì°¨ì˜ ìµœì†Œê°’ì—ì„œ ì¤„ì–´ë“œëŠ” ë³€í™”ëŸ‰ì´ ëª‡ ì´ìƒì¸ì§€ ì§€ì • (ê¸°ë³¸ 0)
        - `patience` : ì˜¤ì°¨ê°€ ì¤„ì§€ ì•ŠëŠ” ìƒí™©ì„ ëª‡ë²ˆ ê¸°ë‹¤ë¦´ì§€ ì§€ì • (ê¸°ë³¸ 0)
3. ê°€ì¤‘ì¹˜ ê·œì œ (Regularization)
    - ì˜¤ì°¨ í•¨ìˆ˜ì— í˜ë„í‹° ì¶”ê°€ í¬í•¨ (íŒŒë¼ë¯¸í„° ì •ë¦¬)
    - L1 ê·œì œ : Lasso
        - ì˜¤ì°¨ í•¨ìˆ˜ = ì˜¤ì°¨ + $$\lambda \sum \vert w \vert $$
            - $$\lambda$$ : ê·œì œ ê°•ë„
        - ê°€ì¤‘ì¹˜(íŒŒë¼ë¯¸í„°) ì ˆëŒ€ê°’ì˜ í•©ì„ ìµœì†Œí™” â†’ ê°€ì¤‘ì¹˜ê°€ ì‘ì€ ê°’ë“¤ì€ 0ìœ¼ë¡œ ë§Œë“œëŠ” ê²½í–¥
    - L2 ê·œì œ : Ridge
        - ì˜¤ì°¨ í•¨ìˆ˜ = ì˜¤ì°¨ + $$\lambda \sum w^2$$
        - ê°€ì¤‘ì¹˜ ì œê³±ì˜ í•©ì„ ìµœì†Œí™”
            - ê·œì œ ê°•ë„ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì˜í–¥ë ¥ì„ ì œì–´
            - ê°•ë„ê°€ í¬ë©´, í° ê°€ì¤‘ì¹˜ê°€ ì¢€ ë” ì¤„ì–´ë“œëŠ” íš¨ê³¼ â†’ ì‘ì€ ê°€ì¤‘ì¹˜ëŠ” 0ì— ìˆ˜ë ´
    - L1, L2 ê·œì œì˜ ê°•ë„ : ì¼ë°˜ì ì¸ ê°’ì˜ ë²”ìœ„
        - **L1 : 0.0001 ~ 0.1**
        - **L2 : 0.001 ~ 0.5**
        - **ê°•ë„ê°€ ë†’ì„ìˆ˜ë¡ â†’ ì¼ë°˜í™”ëœ ëª¨ë¸(ë‹¨ìˆœí•œ ëª¨ë¸)**
        - Hidden Layerì—ì„œ ì§€ì •
            - ëª¨ë“  Hidden Layerì—ì„œ ì§€ì • or ë…¸ë“œì˜ ìˆ˜ê°€ ë§ì€ ì¸µì—ì„œë§Œ ì§€ì • â†’ ìƒí™©ì— ë§ê²Œ ì‚¬ìš©
4. Dropout
    - í›ˆë ¨ ê³¼ì •ì—ì„œ ì‹ ê²½ë§ì˜ ì¼ë¶€ ë‰´ëŸ°ì„ ì„ì˜ë¡œ ë¹„í™œì„±í™” ì‹œí‚´ â†’ ëª¨ë¸ ê°•ì œë¡œ ì¼ë°˜í™”
    - ì ìš© ì ˆì°¨
        - í›ˆë ¨ ë°°ì¹˜ì—ì„œ ëœë¤í•˜ê²Œ ì„ íƒëœ ì¼ë¶€ ë‰´ëŸ°ì„ ì œê±°
        - ì œê±°ëœ ë‰´ëŸ°ì€ í•´ë‹¹ ë°°ì¹˜ì— ëŒ€í•œ ìˆœì „íŒŒ ë° ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ë¹„í™œì„±í™”
        - ì´ë¥¼ í†µí•´ ë‰´ëŸ°ë“¤ ê°„ì˜ ë³µì¡í•œ ì˜ì¡´ì„±ì„ ì¤„ì—¬ ì¤Œ
        - ë§¤ epochs ë§ˆë‹¤ ë‹¤ë¥¸ ë¶€ë¶„ ì§‘í•©ì˜ ë‰´ëŸ°ì„ ë¹„í™œì„±í™” â” ì•™ìƒë¸” íš¨ê³¼
    - Hidden Layer ë‹¤ìŒì— Dropout Layer ì¶”ê°€
    - Dropout Rate
        - 0.4 : hidden layerì˜ ë…¸ë“œ ì¤‘ 40%ë¥¼ ì„ì˜ë¡œ ì œì™¸ì‹œí‚´.
        - ë³´í†µ 0.2 ~ 0.5 ì‚¬ì´ì˜ ë²”ìœ„ ì§€ì •
            - ì¡°ì ˆí•˜ë©´ì„œ ì°¾ì•„ì•¼ í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°!
            - Featureê°€ ì ì„ ê²½ìš° rateë¥¼ ë‚®ì¶”ê³ , ë§ì„ ê²½ìš°ëŠ” rateë¥¼ ë†’ì´ëŠ” ì‹œë„

### ëª¨ë¸ ì €ì¥í•˜ê¸°

- h5 íŒŒì¼ë¡œ ì €ì¥
- ëª¨ë¸ ë¡œë”©: load_model í•¨ìˆ˜ë¡œ ëª¨ë¸ ë¡œë”©
- ê° epoch ë§ˆë‹¤ ëª¨ë¸ ì €ì¥ ê°€ëŠ¥

### ì‹¤ìŠµ

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import *
from sklearn.preprocessing import MinMaxScaler

from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.backend import clear_session
from tensorflow.keras.optimizers import Adam
from keras.datasets import mnist

from keras.callbacks import EarlyStopping
from keras.regularizers import l1, l2
from keras.layers import Dropout
from keras.models import load_model
from keras.callbacks import ModelCheckpoint

# 1. Early Stopping

# ëª¨ë¸ ì„ ì–¸
clear_session()
model2 = Sequential( [Dense(128, input_shape = (nfeatures,), activation= 'relu'),
                      Dense(64, activation= 'relu'),
                      Dense(32, activation= 'relu'),
                      Dense(1, activation= 'sigmoid')] )
model2.compile(optimizer= Adam(learning_rate = 0.001), loss='binary_crossentropy')

# EarlyStopping ì„¤ì • ------------
min_de = 0.005
pat = 5

es = EarlyStopping(monitor = 'val_loss', min_delta = min_de, patience = pat)
# --------------------------------

# í•™ìŠµ
hist = model2.fit(x_train, y_train, epochs = 100, validation_split=0.2,
                  callbacks = [es]).history
dl_history_plot(hist)

# 2.ê°€ì¤‘ì¹˜ ê·œì œ(Regularization)
model4 = Sequential( [Dense(128, input_shape = (nfeatures,), activation= 'relu',
                            kernel_regularizer = l1(0.01)),
                      Dense(64, activation= 'relu',
                            kernel_regularizer = l1(0.01)),
                      Dense(32, activation= 'relu',
                            kernel_regularizer = l1(0.01)),
                      Dense(1, activation= 'sigmoid')] )

model4.compile(optimizer= Adam(learning_rate = 0.001), loss='binary_crossentropy')
hist = model4.fit(x_train, y_train, epochs = 100, validation_split=0.2, verbose = 0).history
dl_history_plot(hist)

# 3. Dropout
model3 = Sequential( [Dense(128, input_shape = (nfeatures,), activation= 'relu'),
                      Dropout(0.4),
                      Dense(64, activation= 'relu'),
                      Dropout(0.4),
                      Dense(32, activation= 'relu'),
                      Dropout(0.4),
                      Dense(1, activation= 'sigmoid')] )

model3.compile(optimizer= Adam(learning_rate = 0.001), loss='binary_crossentropy')
hist = model3.fit(x_train, y_train, epochs = 50, validation_split=0.2, verbose = 0).history
dl_history_plot(hist)

# 4. ëª¨ë¸ ì €ì¥
model1.save('hanky.h5')
model2 = load_model('hanky.h5') # ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥

## ì¤‘ê°„ ì²´í¬ í¬ì¸íŠ¸ì— ëª¨ë¸ ì €ì¥
nfeatures = x_train.shape[1]
model1 = Sequential( [Dense(64, input_shape = (nfeatures,), activation= 'relu'),
                      Dense(32, activation= 'relu'),
                      Dense(16, activation= 'relu'),
                      Dense(1, activation= 'sigmoid')] )

model1.compile(optimizer= Adam(learning_rate = 0.0001), loss='binary_crossentropy')
cp_path = '{epoch:03d}.h5'
mcp = ModelCheckpoint(cp_path, monitor='val_loss', verbose = 1, save_best_only=True)

hist = model1.fit(x_train, y_train, epochs = 50, validation_split=.2, callbacks=[mcp]).history
dl_history_plot(hist)

#ì²´í¬í¬ì¸íŠ¸ ì €ì¥
# ì•„ë˜ ì½”ë“œì—ì„œ ModelCheckpoint ì½œë°±ì€ ê²€ì¦ ë°ì´í„°ì˜ ì •í™•ë„(val_accuracy)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
# ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì´ì „ì— ì €ì¥ëœ ëª¨ë¸ë³´ë‹¤ í–¥ìƒë  ë•Œë§Œ ì €ì¥ë˜ë„ë¡ save_best_only=Trueë¡œ ì„¤ì •
# ë˜í•œ, verbose=1ë¡œ ì„¤ì •í•˜ë©´ ëª¨ë¸ì´ ì €ì¥ë  ë•Œë§ˆë‹¤ ì½˜ì†”ì— ë©”ì‹œì§€ê°€ í‘œì‹œë¨.
```

### ì°¸ê³ 

- íŠœë‹ ì‹œ ì ì ˆí•œ ì§€ì  ì°¾ê¸° : **elbow method** => elbow ì§€ì ì„ ì°¾ê³  ê·¸ ê·¼ë°©ì—ì„œ ë‹µì„ ì°¾ì•„ë¼

## Functional API

### Sequential vs Functional

- Sequential
    - êµ¬ì„±
        - ìˆœì°¨ì ìœ¼ë¡œ ìŒ“ì•„ê°€ë©° ëª¨ë¸ ìƒì„±
        - Input â†’ Output Layer ìˆœì°¨ì  ì—°ê²°
    - ì½”ë“œ
        - ë¦¬ìŠ¤íŠ¸ë¡œ Layer ì…ë ¥
- Functional
    - êµ¬ì„±
        - ëª¨ë¸ì„ ì¢€ ë” ë³µì¡í•˜ê²Œ êµ¬ì„±
        - ëª¨ë¸ì„ ë¶„ë¦¬í•´ì„œ ì‚¬ìš© ê°€ëŠ¥
        - ë‹¤ì¤‘ ì…ë ¥, ë‹¤ì¤‘ ì¶œë ¥ ê°€ëŠ¥
    - ì½”ë“œ
        - Input í•¨ìˆ˜
        - Layer : ì• ë ˆì´ì–´ ì—°ê²° ì§€ì •
        - Model í•¨ìˆ˜ë¡œ ì‹œì‘ê³¼ ë ì—°ê²°í•´ì„œ ì„ ì–¸

### ë‹¤ì¤‘ ì…ë ¥ ëª¨ë¸

- ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì…ë ¥
    - ê° ì…ë ¥ì— ë§ëŠ” íŠ¹ì§• ë„ì¶œ(feature representation) ê°€ëŠ¥

### ì‹¤ìŠµ

```python
from keras.models import Sequential, Model
from keras.layers import Input, Dense, concatenate
from keras.backend import clear_session
from tensorflow.keras.optimizers import Adam

# 1. Sequential
clear_session()

model = Sequential([
            Dense(18 ,input_shape = (nfeatures, ),
                  activation = 'relu' ),
            Dense(4, activation='relu') ,
            Dense(1) ])

model.summary()

# 2. Functional
clear_session()

il = Input(shape=(nfeatures, ))
hl1 = Dense(18, activation='relu')(il)
hl2 = Dense(4, activation='relu')(hl1)
ol = Dense(1)(hl2)

model = Model(inputs = il, outputs = ol)

model.summary()

# 3. ë‹¤ì¤‘ ì…ë ¥ ëª¨ë¸ë§
nfeatures1 = x_train1.shape[1]
nfeatures2 = x_train2.shape[1]

# ëª¨ë¸ êµ¬ì„±
input_1 = Input(shape=(nfeatures1,), name='input_1')
input_2 = Input(shape=(nfeatures2,), name='input_2')

# ì²« ë²ˆì§¸ ì…ë ¥ì„ ìœ„í•œ ë ˆì´ì–´
hl1_1 = Dense(10, activation='relu')(input_1)
# ë‘ ë²ˆì§¸ ì…ë ¥ì„ ìœ„í•œ ë ˆì´ì–´
hl1_2 = Dense(20, activation='relu')(input_2)
# ë‘ íˆë“ ë ˆì´ì–´ ê²°í•©
cbl = concatenate([hl1_1, hl1_2])

# ì¶”ê°€ íˆë“ ë ˆì´ì–´
hl2 = Dense(8, activation='relu')(cbl)
# ì¶œë ¥ ë ˆì´ì–´
output = Dense(1)(hl2)

# ëª¨ë¸ ì„ ì–¸
model = Model(inputs = [input_1, input_2], outputs = output)
model.summary()

# 4. keras_tuner ì„±ëŠ¥ ìµœì í™”
import keras_tuner as kt

def build_model(hp):
    model = Sequential([ Dense(units=hp.Choice('node1', [8, 16, 32, 64, 128, 256]),
                               input_shape = (x_train.shape[1],), activation='relu'),
                         Dense(1)])
    model.compile(loss='mean_absolute_error', optimizer=Adam(learning_rate = hp.Choice('learning_rate', [0.0001, 0.001, 0.01])))
    return model

%%time
tuner = kt.RandomSearch(build_model, objective='val_loss', max_trials = 10, project_name='dnn_tune_2')
tuner.search(x_train, y_train, epochs = 100, validation_split = .2, verbose=0)
best_model = tuner.get_best_models(num_models=1)[0]
tuner.results_summary()

# íŠœë‹ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡í•˜ê³  í‰ê°€í•˜ê¸°
pred2_1 = best_model.predict(x_test, verbose = 0)
print('MAE :', mean_absolute_error(y_test, pred2_1))

plt.scatter(y_test, pred2_1)
plt.plot(y_test, y_test, color = 'gray', linewidth = .5)
plt.grid()
plt.show()
```

## ì‹œê³„ì—´ ëª¨ë¸ë§

### ì‹œê³„ì—´ ë°ì´í„°

- Sequential Data âŠƒ Time Series
    - ìˆœì„œê°€ ìˆë‹¤
    - Sequential + **ì‹œê°„ì˜ ë“±ê°„ê²©**
- ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„
    - **ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ íŒ¨í„´**ì„ ë¶„ì„
    - íë¦„ì„ ì–´ë–»ê²Œ ì •ë¦¬í•˜ëŠ” ì§€ì— ë”°ë¼ì„œ ëª¨ë¸ë§ ë°©ì‹ì´ ë‹¬ë¼ì§

### ì‹œê³„ì—´ ëª¨ë¸ë§ ê°œìš”

- í†µê³„ì  ì‹œê³„ì—´ ëª¨ë¸ë§
    - yì˜ ì´ì „ ì‹œì  ë°ì´í„°ë“¤ë¡œ ë¶€í„° íë¦„ì˜ íŒ¨í„´ì„ ì¶”ì¶œí•˜ì—¬ ì˜ˆì¸¡
        - íŒ¨í„´ : trend, seasonality
        - x ë³€ìˆ˜ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        - íŒ¨í„´ì´ ì¶©ë¶„íˆ ë„ì¶œëœ ëª¨ë¸ì˜ ì”ì°¨ëŠ” Stationary
- ML ê¸°ë°˜ ì‹œê³„ì—´ ëª¨ë¸ë§
    - íŠ¹ì • **ì‹œì ** ë°ì´í„°ë“¤(**1ì°¨ì›**)ê³¼ ì˜ˆì¸¡ ëŒ€ìƒ ì‹œì ($$y_{t+1}$$) ê³¼ì˜ ê´€ê³„ë¡œ ë¶€í„° íŒ¨í„´ì„ ì¶”ì¶œí•˜ì—¬ ì˜ˆì¸¡
        - ì‹œê°„ì˜ íë¦„ì„ xë³€ìˆ˜ë¡œ ë„ì¶œí•˜ëŠ” ê²ƒì´ ì¤‘ìš”
- DL ê¸°ë°˜ ì‹œê³„ì—´ ëª¨ë¸ë§ (**RNN**)
    - **ì‹œê°„ íë¦„ êµ¬ê°„(timesteps)** ë°ì´í„°ë“¤(**2ì°¨ì›**)ê³¼ ì˜ˆì¸¡ ëŒ€ìƒ ì‹œì ($$y_{t+1}$$)  ê³¼ì˜ ê´€ê³„ë¡œ ë¶€í„° íŒ¨í„´ ì¶”ì¶œ
        - ì–´ëŠ ì •ë„ êµ¬ê°„(timesteps)ì„ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ì •í• ì§€ ê²°ì •
        - ë¶„ì„ ë‹¨ìœ„ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ë§Œë“œëŠ” ì „ì²˜ë¦¬ í•„ìš” â†’ ë°ì´í„°ì…‹ì€ 3ì°¨ì›
- ì‹œê³„ì—´ ëª¨ë¸ë§ ì ˆì°¨
    1. y ì‹œê°í™”, ì •ìƒì„± ê²€í† (í†µê³„ì  ëª¨ë¸, **í™”ì´íŠ¸ ë…¸ì´ì¦ˆ: ì”ì°¨ì— íŒ¨í„´ì´ ì—†ëŠ” ìƒíƒœ**)
    2. ëª¨ë¸ ìƒì„±
    3. Train_err(ì”ì°¨) ë¶„ì„ â†’ 2ë²ˆìœ¼ë¡œ ì˜¬ë¼ê°€ê¸° ë°˜ë³µ
    4. ê²€ì¦(ì˜ˆì¸¡)
    5. ê²€ì¦(í‰ê°€) â†’ 2ë²ˆìœ¼ë¡œ ì˜¬ë¼ê°€ê¸° ë°˜ë³µ
- ì‹œê³„ì—´ ëª¨ë¸ í‰ê°€
    - ê¸°ìˆ ì  í‰ê°€
        - ì”ì°¨
            - ACF, PACF
            - ê²€ì • : ì •ìƒì„± ê²€ì •, ì •ê·œì„± ê²€ì •, â€¦
        - ML Metric
            - AIC
            - MAE, MAPE, R2
    - ë¹„ì¦ˆë‹ˆìŠ¤ í‰ê°€
        - ìˆ˜ìš”ëŸ‰ ì˜ˆì¸¡
            - ì¬ê³  íšŒì „ìœ¨
            - í‰ê·  ì¬ê³  ë¹„ìš©

- ì”ì°¨ ë¶„ì„
    - ì”ì°¨(Residual) = ì‹¤ì œ ë°ì´í„° - ì˜ˆì¸¡ê°’
    - ì‹œê³„ì—´ ëª¨ë¸ $$y = f(x) +\epsilon$$
        - ì”ì°¨ $$\epsilon$$ ëŠ” White Noiseì— ê°€ê¹Œì›Œì•¼ í•¨
            - ê°€ê¹ì§€ ì•Šë‹¤ë©´ yì˜ íŒ¨í„´ì„ ì œëŒ€ë¡œ ë°˜ì˜ ëª»í•¨ â†’ ë” í• ì¼ ë‚¨ìŒ
    - ì”ì°¨ ë¶„ì„
        - ì‹œê°í™”: ACF, PACF
        - ê²€ì •
            - ì •ìƒì„± ê²€ì •(ADF Test, **KPSS Test**)
            - ì •ê·œì„± ê²€ì •(Shapiro-wilk Test)
            - ìê¸°ìƒê´€ ê²€ì •(Ljung-Box Test)
            - ë“±ë¶„ì‚°ì„± ê²€ì •(G-Q Test)

## ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ëª¨ë¸ë§(RNN)

### RNN

![Untitled](https://cdn-images-1.medium.com/v2/resize:fit:1000/1*d_POV7c8fzHbKuTgJzCxtA.gif)

### RNN: ì‹œê³„ì—´ ë°ì´í„° ëª¨ë¸ë§

- ìµœê·¼ 4ì¼ê°„ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒë‚  ì£¼ê°€ ì˜ˆì¸¡
    - ìµœê·¼ 4ì¼ê°„ì˜ ì£¼ê°€, ê±°ë˜ëŸ‰, í™˜ìœ¨, ìœ ê°€ì˜ íë¦„ì„ í•™ìŠµí•´ì„œ ë‹¤ìŒë‚  ì£¼ê°€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸
    - ğ‘¥0, ğ‘¥1, ğ‘¥2, ğ‘¥3 : input
    - â„0, â„1, â„2, â„3 : hidden state (ì¤‘ê°„ ê²°ê³¼ë¬¼)
- ê³¼ê±°ì˜ ì •ë³´ë¥¼ í˜„ì¬ì— ë°˜ì˜í•´ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„
- **ì£¼ì˜ ì‚¬í•­: ì‚¬ì „ í™•ì¸ ì˜¤ë¥˜ë¥¼ ë²”í•˜ì§€ ë§ì•„ì•¼ í•œë‹¤**
- ë°ì´í„° ì „ì²˜ë¦¬
    1. ë°ì´í„° ë¶„í•  1: x, y
    2. ìŠ¤ì¼€ì¼ë§
        - X ìŠ¤ì¼€ì¼ë§ í•„ìˆ˜
        - yê°’ì´ í¬ë©´ ìµœì í™”ë¥¼ ìœ„í•´ ìŠ¤ì¼€ì¼ë§ í•„ìš” â†’ ë‹¨, ëª¨ë¸ í‰ê°€ ì‹œ ì›ë˜ ê°’ìœ¼ë¡œ ë³µì›
    3. 3ì°¨ì› ë°ì´í„°ì…‹ ë§Œë“¤ê¸°
        - 2ì°¨ì› ë°ì´í„°ì…‹(x) â†’ timesteps ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ (í•œì¹¸ì”© ë°€ë©´ì„œ, sliding window)
    4. ë°ì´í„° ë¶„í• 2 : train, val

- SimpleRNN
    - ë…¸ë“œ ìˆ˜ 1ê°œ â” ë ˆì´ì–´ì˜ ì¶œë ¥ í˜•íƒœ : timesteps * ë…¸ë“œ ìˆ˜
    - **return_sequences : ì¶œë ¥ ë°ì´í„°ë¥¼ ë‹¤ìŒ ë ˆì´ì–´ì— ì „ë‹¬í•  í¬ê¸° ê²°ì •**
        - True : ì¶œë ¥ í¬ê¸° ê·¸ëŒ€ë¡œ ì „ë‹¬ â” timesteps * nodeìˆ˜
        - False : ê°€ì¥ ë§ˆì§€ë§‰(ìµœê·¼) hidden state ê°’ë§Œ ì „ë‹¬ â” 1 * node ìˆ˜
        
        - **ë§ˆì§€ë§‰ RNN Layerë¥¼ ì œì™¸í•œ ëª¨ë“  RNN Layer : True**
        - **ë§ˆì§€ë§‰ RNN Layer : Falseì™€ True ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥**
            - **ë‹¨, Trueë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ Flattenìœ¼ë¡œ í¼ì¹œ í›„ Dense Layer ë¡œ ì—°ê²°**
                - Flatten: ì¤‘ê°„ ê³¼ì •ì˜ hidden state ê°’ë“¤ì„ 2ì°¨ì›ì—ì„œ 1ì°¨ì›ìœ¼ë¡œ í¼ì¹˜ê¸°

- RNNì˜ í™œì„±í™” í•¨ìˆ˜: tanh(hyperbolic tangent)
    - í•˜ì´í¼ ë³¼ë¦­ íƒ„ì  íŠ¸ í•¨ìˆ˜
    - gradient ì†Œì‹¤ ë¬¸ì œ ì™„í™”
        - gradient ì†Œì‹¤ ë¬¸ì œ: ì—­ì „íŒŒ ì‹œì— gradientê°€ ì‘ì•„ì ¸ í•™ìŠµì´ ì–´ë ¤ì›Œì§€ëŠ” í˜„ìƒ
        - ì‹œê·¸ëª¨ì´ë“œì— ë¹„í•´ gradientë¥¼ ì¢€ ë” í¬ê²Œ ìœ ì§€

## LSTM(Long Short - Term Memory)

### RNNì˜ ë¬¸ì œ

- RNNì˜ Vanishing Gradient ë¬¸ì œ : ê´€ë ¨ ì •ë³´ì™€ ê·¸ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ëŠ” ì§€ì  ì‚¬ì´ ê±°ë¦¬ê°€ ë©€ ê²½ìš° gradient ê°ì†Œë¡œ í•™ìŠµ ëŠ¥ë ¥ì´ í¬ê²Œ ì €í•˜
- RNNì˜ ì¥ê¸° ì˜ì¡´ì„±(long-term dependencies) ë¬¸ì œ: ê¸´ ê¸°ê°„ ë™ì•ˆì˜ ì •ë³´ë¥¼ ìœ ì§€í•˜ê³  í™œìš©í•˜ëŠ” ë° ì–´ë ¤ì›€ ë°œìƒ
1. Cell State ì—…ë°ì´íŠ¸
    - Forget Gate : ë¶ˆí•„ìš”í•œ ê³¼ê±° ìŠê¸°
    - Input Gate : í˜„ì¬ ì •ë³´ ì¤‘ì—ì„œ ì¤‘ìš”í•œ ê²ƒ ê¸°ì–µ
    - Cell state ì—…ë°ì´íŠ¸ : ìœ„ì˜ ë‘ ê°œë¥¼ ê²°í•©í•˜ì—¬ ì¥ê¸° ê¸°ì–µ ë©”ëª¨ë¦¬ì— íƒœìš°ê¸°
2. Hidden State ì—…ë°ì´íŠ¸
    - Hidden StateëŠ” ê³¼ê±°ì˜ hidden stateë¥¼ ë°›ê³  í˜„ì‹œì ì˜ ì…ë ¥ ë°ì´í„°ë¥¼ ê³ ë ¤í•˜ì—¬ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì—­í• 
    - output gate :  `ì—…ë°ì´íŠ¸ ëœ Cell State`ì™€ `input, ì´ì „ ì…€ì˜ hidden state`ë¡œ ìƒˆ hidden state ê°’ ìƒì„±í•´ì„œ ë„˜ê¸°ê¸°

### ì‹¤ìŠµ

```python
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN, LSTM, Flatten
from keras.backend import clear_session
from tensorflow.keras.optimizers import Adam

# ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬ 2ì°¨ì› --> 3ì°¨ì›ìœ¼ë¡œ ë³€í™˜
def temporalize(x, y, timesteps):
    nfeature = x.shape[1]
    output_x = []
    output_y = []
    for i in range(len(x) - timesteps + 1):
        t = []
        for j in range(timesteps):
            t.append(x[[(i + j)], :])
        output_x.append(t)
        output_y.append(y[i + timesteps - 1])
    return np.array(output_x).reshape(-1,timesteps, nfeature), np.array(output_y)

# 1. RNN ëª¨ë¸ë§
timesteps, nfeatures = x_train.shape[1], x_train.shape[2]

clear_session()

model = Sequential([SimpleRNN(8, input_shape = (timesteps, nfeatures), return_sequences = True), 
                           SimpleRNN(8), Dense(1)])
model.summary()

# 2. LSTM ëª¨ë¸ë§
clear_session()

model = Sequential([LSTM(8, input_shape = (timesteps, nfeatures), return_sequences = True), 
                           LSTM(8), Dense(1)])
model.summary()
```

## ì°¸ì¡°

### ì°¨ì›ì˜ ì €ì£¼, ì°¨ì› ì¶•ì†Œ

- ì°¨ì›ì˜ ì €ì£¼ : ë³€ìˆ˜ì˜ ìˆ˜(ì°¨ì›ì˜ ìˆ˜)ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ë°ì´í„°ê°€ í¬ë°•í•´ì§„ë‹¤ â†’ í•™ìŠµì´ ì ì ˆí•˜ê²Œ ë˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§
- í¬ë°•í•œ ë°ì´í„° ë¬¸ì œ í•´ê²°ë°©ì•ˆ
    - í–‰ ëŠ˜ë¦¬ê¸° : ë°ì´í„° ëŠ˜ë¦¬ê¸°
    - ì—´ ì¤„ì´ê¸° : ì°¨ì› ì¶•ì†Œ
- ì°¨ì› ì¶•ì†Œ
    - ê¸°ì¡´ íŠ¹ì„±ì„ ìµœëŒ€í•œ ìœ ì§€í•œ ìƒíƒœë¡œ ë‹¤ìˆ˜ì˜ ê³ ì°¨ì› featureë“¤ì„ ì†Œìˆ˜ì˜ ì €ì°¨ì› featureë¡œ ì¶•ì†Œ
    - ì£¼ì„±ë¶„ ë¶„ì„(PCA), t-SNE
    1. PCA
        - ë³€ìˆ˜ì˜ ìˆ˜ë³´ë‹¤ ì ì€ ì €ì°¨ì› í‰ë©´ìœ¼ë¡œ íˆ¬ì˜
        - ë¶„ì‚°ì„ ìµœëŒ€í•œ ìœ ì§€í•˜ë©´ì„œ ì°¨ì› ì¶•ì†Œ
        - ì ˆì°¨
            1. í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ ë¶„ì‚°ì´ ìµœëŒ€ì¸ ì²«ë²ˆì§¸ ì¶•(axis)ì„ ì°¾ìŒ
            2. ì²«ë²ˆì§¸ ì¶•ê³¼ ì§êµ(orthogonal)í•˜ë©´ì„œ ë¶„ì‚°ì´ ìµœëŒ€ì¸ ë‘ ë²ˆì§¸ ì¶•ì„ ì°¾ìŒ
            3. ì²« ë²ˆì§¸ ì¶•ê³¼ ë‘ ë²ˆì§¸ ì¶•ì— ì§êµí•˜ê³  ë¶„ì‚°ì´ ìµœëŒ€ì¸ ì„¸ ë²ˆì§¸ ì¶•ì„ ì°¾ìŒ
            4. 1 ~ 3ë²ˆê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ë°ì´í„°ì…‹ì˜ ì°¨ì› ë§Œí¼ì˜ ì¶•ì„ ì°¾ìŒ
        - ë¶„ì„ ìˆ˜í–‰ í›„, ê° ì¶•ì˜ ë‹¨ìœ„ë²¡í„° : **ì£¼ì„±ë¶„**(ê° ì¶• ë³„ íˆ¬ì˜ëœ ê°’ ì €ì¥ë¨)
        - PCA ì‚¬ìš©í•˜ê¸°
            - ì „ì²˜ë¦¬ : ìŠ¤ì¼€ì¼ë§ í•„ìš”
            - PCA ë¬¸ë²•
                - ì£¼ì„±ë¶„ì˜ ê°œìˆ˜ ì§€ì • í›„ fit & transform
                    - ê°œìˆ˜ë¥¼ ëŠ˜ë ¤ê°€ë©´ì„œ ì›ë³¸ ë°ì´í„° ë¶„ì‚°ê³¼ ë¹„êµ(elbow method)