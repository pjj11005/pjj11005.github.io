---
layout: post
title: 핸즈온 머신러닝 2판 | 3. 분류
categories: 
  - mldl
  - handsonml2
description: 핸즈온 머신러닝 2판에서 공부했던 내용을 정리하는 부분입니다.
sitemap: false
---

MNIST 데이터셋을 통해 분류시스템 집중적으로 분석

* this unordered seed list will be replaced by the toc
{:toc}

## 3.1 MNIST

- 고등학생과 미국 조사국 직원들이 손으로 쓴 70,000개의 작은 숫자 이미지 데이터셋
- 머신러닝 분야의 ‘Hello World’, 분류 학습용으로 많이 쓰임
- 사이킷런에서 읽어들인 데이터셋의 특징
    - DESCR 키: 데이터셋을 설명
    - data 키: 샘플이 행, 특성이 열로 이루어진 배열
    - target 키: 레이블 배열
- 이미지: 70,000개 / 특성: 784개 / 이미지 28 X 28 픽셀
- 훈련 세트: 앞쪽 60,000개 / 테스트 세트: 뒤쪽 10,000개 → 훈련 세트 이미 섞여서 모든 교차 검증 폴드 비슷하게 만듦
- 어떤 학습 알고리즘은 훈련 샘플 순서에 민감 → 데이터셋을 섞으면 이런 문제 방지 가능

## 3.2 이진 분류기 훈련

- 이진 분류기 → ex) 5와 5아님 두개의 클래스로 분류
- SGD 분류기(Stochastic Gradient Descent Classifier)
    - 매우 큰 데이터 셋을 효율적으로 처리 → 한번에 하나씩 훈련 샘플을 독립적으로 처리하기 때문(온라인 학습에 적합)
    - Tip: SGD classifier는 훈련에 무작위성 사용 → 결과 재현을 위해서는 random_state 매개변수 지정

## 3.3 성능 측정

- 3.3.1 교차 검증을 사용한 정확도 측정
    - 교차 검증 구현
        - StratifiedkFold
            
            ```python
            from sklearn.model_selection import StratifiedKFold
            from sklearn.base import clone
            
            # shuffle=False가 기본값이기 때문에 random_state를 삭제하던지 shuffle=True로 지정하라는 경고가 발생합니다.
            # 0.24버전부터는 에러가 발생할 예정이므로 향후 버전을 위해 shuffle=True을 지정합니다.
            skfolds = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)
            
            for train_index, test_index in skfolds.split(X_train, y_train_5):
                clone_clf = clone(sgd_clf)
                X_train_folds = X_train[train_index]
                y_train_folds = y_train_5[train_index]
                X_test_fold = X_train[test_index]
                y_test_fold = y_train_5[test_index]
            
                clone_clf.fit(X_train_folds, y_train_folds)
                y_pred = clone_clf.predict(X_test_fold)
                n_correct = sum(y_pred == y_test_fold)
                print(n_correct / len(y_pred))
            ```
            
            - 클래스별 비율이 유지되도록 폴드를 만들기 위해 계층적 샘플링 수행
    - cross_val_score() 3개의 폴드 교차 검증으로 SGD classifier 평가 → 모든 교차 검증 폴드에 대해 95%이상의 정확도
        
        ```python
        from sklearn.model_selection import cross_val_score
        cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")
        ```
        
    - 모든 이미지 ‘5아님’으로 분류하는 더미 분류기
        
        ```python
        from sklearn.base import BaseEstimator
        class Never5Classifier(BaseEstimator):
            def fit(self, X, y=None):
                pass
            def predict(self, X):
                return np.zeros((len(X), 1), dtype=bool)
        
        never_5_clf = Never5Classifier()
        cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring="accuracy")
        ```
        
        - 정확도 90%이상으로 나옴 → 이미지의 10% 정도만 숫자 5이기 때문에 무조건 ‘5 아님’으로 예측하면 정확도 90%
        - 이 예제는 정확도를 분류기의 성능 측정 지표로 사용하지 않는 이유 보여줌 → 특히 불균형한 데이터셋 사용 시 더욱 그렇다.
    >- 노트: 이 출력(그리고 이 노트북과 다른 노트북의 출력)이 책의 내용과 조금 다를 수 있다.
    >    - 첫째, 사이킷런과 다른 라이브러리들이 발전하면서 알고리즘이 조금씩 변경되기 때문에 얻어지는 결괏값이 바뀔 수 있다. 최신 사이킷런 버전을 사용한다면(일반적으로 권장됩니다) 책이나 이 노트북을 만들 때 사용한 버전과 다를 것이므로 차이가 남
    >    - 둘째, 많은 훈련 알고리즘은 확률적이다. 즉 무작위성에 의존한다. 이론적으로 의사 난수를 생성하도록 난수 생성기에 시드 값을 지정하여 일관된 결과를 얻을 수 있다(random_state=42나 np.random.seed(42)를 종종 보게 되는 이유). 하지만 여기에서 언급한 다른 요인으로 인해 충분하지 않을 때가 있다.
    >    - 셋째, 훈련 알고리즘이 여러 스레드(C로 구현된 알고리즘)나 여러 프로세스(예를 들어 n_jobs 매개변수를 사용할 때)로 실행되면 연산이 실행되는 정확한 순서가 항상 보장되지 않는다. 따라서 결괏값이 조금 다를 수 있다.
    >    - 마지막으로, 여러 세션에 결쳐 순서가 보장되지 않는 파이썬 딕셔너리(dict)이나 셋(set) 같은 것은 완벽한 재현성이 불가능하다. 또한 디렉토리 안에 있는 파일의 순서도 보장되지 않는다.
- 3.3.2 오차 행렬
    - 분류기 성능 평가로 더 좋은 방법: 클래스 A의 샘플이 클래스 B로 분류된 횟수 측정
        - ex) 숫자 5의 이미지를 3으로 잘못 분류한 횟수 → 오차 행렬 5행 3열
    - 오차 행렬 만들려면 예측 값이 있어야함 → cross_val_predict() 함수 사용
        - 교차검증 수행하여 각 테스트 폴드에서 얻은 예측 반환
    - 오차 행렬의 구성
        - 행: 실제 클래스 / 열: 예측 클래스
            
            
            | TN(진짜 음성) | FP(거짓양성) |
            | --- | --- |
            | FN(거짓 음성) | TP(진짜 양성) |
        - 정밀도(Precision): 양성 예측의 정확도

            $$
            \begin{align*}
            & 정밀도 = \frac{TP}{TP+FP}
            \end{align*}
            $$

        - 재현율(Recall): 분류기가 정확하게 감지한 양성 샘플의 비율 → 민감도(sensitivity), 진짜양성 비율(TPR)
            
            $$
            \begin{align*}
            & 정밀도 = \frac{TP}{TP+FN}
            \end{align*}
            $$

- 3.3.3 정밀도와 재현율
    - precision: 0.729, recall: 0.755 → 5로 판별된 이미지 중에서 72.9%만 정확, 전체 숫자 5에서 75.6%만 감지
    - F1 score
        - 두 분류기 비교할 때 좋음
        
        $$
        \begin{align*}
        & F1 = 2\times\frac{정밀도 \times 재현율}{정밀도 + 재현율} = \frac{TP}{TP+\frac{FN+FP}{2}}
        \end{align*}
        $$
        
        - 정밀도, 재현율 비슷하면 F1 score 높다 → 항상 바람직한 것은 아님
        - 정밀도 재현율 트레이드오프: 정밀도를 올리면 재현율이 낮아지고 그 반대도 일어나는 것
- 3.3.4 정밀도/재현율 트레이드 오프
    - SGDClassifier 분류 진행 방법과 트레이드오프 이해
        - 결정 함수(decision function)을 사용하여 각 샘플의 점수 계산 → 사이킷런의 decision_function()함수로 예측에 사용한 점수 확인 가능
        - 결정 임곗값(decision threshold): 이 값보다 점수가 크면 양성 클래스, 작으면 음성 클래스
        - 적절한 임계값 정하기
            
            ```python
            from sklearn.metrics import precision_recall_curve
            
            y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,
                                         method="decision_function")
            
            precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)
            
            def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):
                plt.plot(thresholds, precisions[:-1], "b--", label="Precision", linewidth=2)
                plt.plot(thresholds, recalls[:-1], "g-", label="Recall", linewidth=2)
                plt.legend(loc="center right", fontsize=16) # Not shown in the book
                plt.xlabel("Threshold", fontsize=16)        # Not shown
                plt.grid(True)                              # Not shown
                plt.axis([-50000, 50000, 0, 1])             # Not shown
            
            recall_90_precision = recalls[np.argmax(precisions >= 0.90)]
            threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]
            
            plt.figure(figsize=(8, 4))                                                                  # Not shown
            plot_precision_recall_vs_threshold(precisions, recalls, thresholds)
            plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], "r:")                 # Not shown
            plt.plot([-50000, threshold_90_precision], [0.9, 0.9], "r:")                                # Not shown
            plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], "r:")# Not shown
            plt.plot([threshold_90_precision], [0.9], "ro")                                             # Not shown
            plt.plot([threshold_90_precision], [recall_90_precision], "ro")                             # Not shown
            save_fig("precision_recall_vs_threshold_plot")                                              # Not shown
            plt.show()
            ```
            
            - cross_val_predict() 함수로 모든 샘플의 점수 구함
            - 임계값의 함수로 정밀도와 재현율 그림
                
                ![Untitled](/assets/img/blog/precision_recall_vs_threshold_plot.png)
                
                - NOTE
                    - 정밀도 곡선이 재현율 곡선보다 울퉁불퉁한 이유 → 임계값을 올리더라도 정밀도가 가끔 낮아질 때가 있다
            - 좋은 정밀도/재현율 트레이드오프 선택 방법
                
                ![Untitled](/assets/img/blog/precision_vs_recall_plot.png)
                
                - 재현율에 대한 정밀도 곡선 그리기
                - average_precision_score() 함수를 사용하면 정밀도/재현율 곡선 아래 면적을 구할 수 있어서 서로 다른 두 모델 비교에 좋음
                - 재현율 80%근처에서 정밀도 급격히 감소 → 이 하강지점 근




## **참고 문헌 및 사이트** 

- 핸즈온 머신러닝 2판