---
layout: post
title: 핸즈온 머신러닝 2판 | 2. 머신러닝 프로젝트 처음부터 끝까지
categories: 
  - mldl
  - handsonml2
description: 핸즈온 머신러닝 2판에서 공부했던 내용을 정리하는 부분입니다.
sitemap: false
---

부동산 회사에 막 고용된 데이터 과학자라고 가정 후 프로젝트 진행

* this unordered seed list will be replaced by the toc
{:toc}

## 2.1 실제 데이터로 작업하기

- 실제 데이터로 실험하는 것이 가장 좋다
- StatLib 저장소의 캘리포니아 주택 가격 데이터셋 사용

## 2.2 큰 그림 보기

- 캘리포니아 인구 조사 데이터로 주택 가격 모델 만들기
- 캘리포니아 블록 그룹마다 인구, 중간 소득, 중간 주택 가격을 담고 있다.
- 구역의 중간 주택 가격 예측

### 2.2.1 문제 정의

- 비즈니스의 목적을 정확히 아는 것이 중요
- 파이프라인
    - 데이터 처리 ‘컴포넌트’들이 연속되어 있는 것
    - 보통 컴포넌트들은 비동기적으로 동작(각 컴포넌트 완전히 독립)
- 지도학습(레이블된 훈련 샘플 존재), 회귀(다중 회귀, 단변량 회귀), 배치학습

### 2.2.2 성능 측정 지표 선택

- 평균 제곱근 오차(RMSE)
    - 회귀 문제의 전형적인 성능 지표
    - 유클리디안 노름(Euclidean norm), l2노름
- 평균 절대 오차(평균 절대 편차, MAE)
    - 맨해튼 노름(Manhattan norm), l1노름
- 노름의 지수가 클수록 큰 값에 치우쳐진다.
- RMSE가 MAE보다 이상치에 민감하다.
- 이상치가 드물면 RMSE가 맞아 일반적으로 널리 사용

### 2.2.3 가정 검사

- 마지막으로 지금까지의 가정들을 나열하고 검사하는 것이 좋다.

## 2.3 데이터 가져오기

### 2.3.1 작업환경 만들기

- 아나콘다 python=3.7, tensorflow-gpu=2.6.0(gpu 사용)의 test3.7 가상환경 생성

### 2.3.2 데이터 다운로드

- housing.tgz 다운로드 및 추출 함수
    
    ```python
    import os
    import tarfile
    import urllib.request
    
    DOWNLOAD_ROOT = "https://raw.githubusercontent.com/rickiepark/handson-ml2/master/"
    HOUSING_PATH = os.path.join("datasets", "housing")
    HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"
    
    def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
        if not os.path.isdir(housing_path):
            os.makedirs(housing_path)
        tgz_path = os.path.join(housing_path, "housing.tgz")
        urllib.request.urlretrieve(housing_url, tgz_path)
        housing_tgz = tarfile.open(tgz_path)
        housing_tgz.extractall(path=housing_path)
        housing_tgz.close()
    ```
    

### 2.3.3 데이터 구조 훑어보기

- 데이터의 특성(10개)
    - ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value', 'ocean_proximity']
- info(): 데이터에 대한 간략한 설명, 전체 행 수, 각 특성의 데이터 타입, 널이 아닌 값의 개수 확인에 유용
- describe(): 숫자형 특성의 요약 정보 보여줌
- hist(): 데이터의 형태를 빠르게 보기 위해 각 숫자형 특성을 히스토그램으로 그려본다.
    
    ```python
    %matplotlib inline
    import matplotlib.pyplot as plt
    housing.hist(bins=50, figsize=(20,15))
    save_fig("attribute_histogram_plots")
    plt.show()
    ```
    
    - %matplotlib inline: 주피터 자체 백엔드를 사용하도록 지정 → IPython kernel 4.4.0, matplotlib 1.5.0 이상부터는 자동으로 주피터 자체 백엔드로 설정
    - 몇가지 사항 확인
        1. 중간 소득(median income) 특성 US달러로 표현 X → 스케일 조정과 상한 15, 하한 3으로 조정(ex) 3은 실제로 30,000달러 의미)
        2. 중간 주택 연도(housing_median_age), 중간 주택 가격(median_house_value)의 최대,최소값도 한정 → 중간 주택 가격은 타켓 변수로 두가지 선택 방법이 필요
            1. 한계값 밖의 구역에 대한 정확한 레이블 구함
            2. 훈련 세트에서 이런 구역 제거($500,000가 넘는 값에 대한 예측은 평가 결과가 나쁘다고 보고 테스트 세트에서도 제거)
        3. 특성들의 스케일이 서로 많이 다르다.
        4. 많은 히스토그램들의 꼬리가 두껍다 → 나중에 종 모양의 분포로 변형 필요
    - 데이터를 깊게 들여다가 보기 전에 테스트 세트를 따로 두고 절대 참고하면 안됨

### 2.3.4 테스트 세트 만들기

- 데이터 스누핑 편향(data snooping): 테스트 세트로 파악한 패턴에 맞는 머신러닝 모델을 선택하여 기대한 성능이 나오지 않는 것
- 무작위 샘플링
    - 사이킷런의 train_test_split
        - random_state: 난수 초깃값 지정 매개변수
        - 행의 개수가 같은 여러 개의 데이터 셋을 넘겨서 인덱스 기반으로 나눌 수 있다. (데이터 프레임이 레이블에 따라 여러 개로 나누어져 있을 때 매우 유용)
- 계층적 샘플링: 계층이라는 동질의 그룹으로 나뉘고 테스트 세트가 전체를 대표하도록 그룹 별로 올바른 수의 샘플을 추출
- 중간 소득이 중간 주택 가격 예측의 중요 변수라고 가정
    - 소득에 대한 카테고리 특성 생성
    - 사이킷런의 StratifiedShuffleSplit을 사용하여 소득 카테고리 기반으로 계층 샘플링
        
        ```python
        from sklearn.model_selection import StratifiedShuffleSplit
        
        split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
        for train_index, test_index in split.split(housing, housing["income_cat"]):
            strat_train_set = housing.loc[train_index]
            strat_test_set = housing.loc[test_index]
        ```
        
    - StratifiedShuffleSplit
        - StratifiedKFold의 계층 샘플링과 ShuffleSplit의 랜덤 샘플링을 합친 것
        - 매개변수 test_size+train_size의 합을 1이하로 지정 가능
- 계층 샘플링의 경우 전체 데이터셋의 소득 카테고리 비율과 거의 유사
- 일반 무작위 샘플링은 많이 다르다

## 2.4 데이터 이해를 위한 탐색과 시각화

- 훈련 세트에 대해서만 탐색
- 훈련 세트의 크기가 매우 크면 별도로 샘플링할 수 도 있음
- 복사본 만들어 탐색

### 2.4.1 지리적 데이터 시각화

- 주택 가격은 지역과 인구 밀집에 관련이 매우 크다.

### 2.4.2 상관관계 조사

- 상관 계수
    - 선형적인 상관관계만 측정(비선형적 관계 알 수 없다)
    - 상관계수 수치와 기울기는 관련성이 없다
- 중간 주택 가격(median_house_value)와 중간 소득(median income)의 상과관계 산점도
    - 상관관계 매우 강함
    - 앞서 본 $500,000과 $450,000, $350,000, $280,000에서 수평선의 분포 보임 → 이런 이상한 형태를 학습하지 않도록 해당 구역을 제거하는 것이 좋다.

### 2.4.3 특성 조합으로 실험

- 가구당 방 개수, 침실/방, 가구당 인원 등의 유용해 보이는 특성 생성
- 침실/방, 가구당 방 개수 특성들은 기존의 특성들보다 중간 주택 가격과의 상관관계가 높다.
- 특히 머신러닝 프로젝트에서는 빠른 프로토 타이핑과 반복적인 프로세스가 권장됨

## 2.5 머신러닝 알고리즘을 위한 데이터 준비

- 이 작업을 수동으로 하는 대신 자동화 함수를 생성해야하는 이유
    - 어떤 데이터 셋에 대해서도 데이터 변환을 쉽게 반복 가능(ex) 다음번에 새로운 데이터셋 사용할 때)
    - 향후 프로젝트에 사용 가능한 변환 라이브러리 점진적 구축
    - 실제 시스템에서 알고리즘에 새 데이터 주입 전 변환 시 사용 가능
    - 여러가지 데이터 변화 쉽게 시도 가능하고, 어떤 조합이 가장 좋은지 확인하는데 편리
- 예측 변수, 레이블 분리

### 2.5.1 데이터 정제

- 누락된 특성 처리
    - 해당 구역 제거: dropna()
    - 전체 특성 삭제: drop()
    - 어떤 값으로 채움(0, 평균, 중간 값 등): fillna() → 계산한 값을 기억해서 테스트 세트의 누락된 값에도 적용해야 한다.
        
        ```python
        housing.dropna(subset=["total_bedrooms"])    # 옵션 1
        housing.drop("total_bedrooms", axis=1)       # 옵션 2
        median = housing["total_bedrooms"].median()  # 옵션 3
        housing["total_bedrooms"].fillna(median, inplace=True)
        ```
        
    - 사이킷런의 SimpleImputer
        - 각 특성의 중간 값 계산해서 객체의 statistics 속성에 저장
        - 모든 수치형 특성에 imputer 적용하는것이 바람직(새로운 데이터에서 어떤 값이 누락될지 모르기 때문)
            
            ```python
            from sklearn.impute import SimpleImputer
            imputer = SimpleImputer(strategy="median")
            housing_num = housing.drop("ocean_proximity", axis=1)
            # 다른 방법: housing_num = housing.select_dtypes(include=[np.number])
            imputer.fit(housing_num)
            
            imputer.statistics_ #array([-118.51, 34.26, 29., 2119., 433., 1164., 408., 3.54155])
            housing_num.median().values #array([-118.51, 34.26, 29., 2119., 433., 1164., 408., 3.54155])
            
            X = imputer.transform(housing_num)
            housing_tr = pd.DataFrame(X, columns=housing_num.columns,
                                      index=housing_num.index)
            housing_tr.head()
            ```
            
- 사이킷런의 설계 철학
    - 일관성: 모든 객체가 일관되고 단순한 인터페이스 공유
        - 추정기(estimator)
            - 데이터셋 기반으로 일련의 모델 파라미터들을 추정하는 객체 ex) imputer
            - fit() 메서드에 의해 추정 수행되고 하나의 매개변수로 하나의 데이터셋만 전달
        - 변환기(transformer)
            - (imputer 같이) 데이터셋을 변환하는 추정기
            - 데이터셋을 매개변수로 전달받은 transform() 메서드가 변환 수행
            - fit_transform: fit()과 transform()을 연달아 호출하는 것과 동일
        - 예측기(predictor)
            - 일부 추정기는 주어진 데이터셋에 대해 예측을 만들 수 있다 ex) LinearRegression 모델 → 예측기
            - predict(): 새로운 데이터셋을 받아 이에 상응하는 예측값을 반환
            - score(): 테스트 세트를 사용해 예측의 품질을 측정함
    - 검사 가능
        - 모든 추정기의 하이퍼파라미터는 공개 인스턴스 변수로 직접 접근 가능 ex) imputer.strategy
        - 모든 추정기의 학습된 모델파라미터 접미사로 밑줄을 붙여서 공개 인스턴스 변수로 제공 ex) imputer.statistics_
    - 클래스 남용 방지: 데이터셋을 넘파이 배열, 사이파이 희소 행렬로 표현
    - 조합성: 기존의 구성요소 최대한 재사용 ex) 여러 변환기를 연결한 다음 마지막에 추정기 하나를 배치한 Pipeline 추정기
    - 합리적인 기본 값: 대부분의 매개변수에 합리적인 기본값 지정 → 일단 돌아가는 기본 시스템 빠르게 만들 수 있다.

### 2.5.2 텍스트와 범주형 특성 다루기

- OrdinalEncoder
    - 범주형 특성 값을 숫자로 변환
    - 문제: 머신러닝 알고리즘이 가까운 값을 더 비슷하다고 생각
- OneHotEncoder
    - 한 특성만 1, 나머지는 모두 0 (더미 특성: 새로운 특성)
    - 사이파이 희소 행렬로 출력 → numpy 배열로 바꾸려면 toarray() 메서드 사용
    - 0이 아닌 원소의 위치만 저장하므로 카테고리의 수가 만을 때 매우 효율적
- 둘다 인코더의 categories_ 인스턴스 변수를 사용해 카테고리 리스트를 얻을 수 있다.
- Tip
    - 카테고리 특성이 많으면 원-핫 인코딩은 많은 수의 입력 특성 생성 → 훈련을 느리고 성능 감소
    - 범주형 입력 값 이 특성과 관련된 숫자형 특성으로 변경 ex) ocean proximity 특성 해안까지의 거리로 변경
    - 표현학습
        - 각 카테고리를 임베딩으로 바꿔 훈련하는 동안 각 카테고리의 표현 학습
        - 임베딩: 학습 가능한 저차원의 벡터로 변경한 결과 혹은 그 과정 전체

### 2.5.3 나만의 변환기

- 덕타이핑: 상속이나 인터페이스 구현이 아닌 객체의 속성이나 메서드가 객체의 유형을 결정하는 방식
- 사이킷런은 덕타이핑 지원
- 앞의 조합 특성을 추가하는 간단한 변환기
    
    ```python
    from sklearn.base import BaseEstimator, TransformerMixin
    
    # 열 인덱스
    rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6
    
    class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
        def __init__(self, add_bedrooms_per_room=True): # *args 또는 **kargs 없음
            self.add_bedrooms_per_room = add_bedrooms_per_room
        def fit(self, X, y=None):
            return self  # 아무것도 하지 않습니다
        def transform(self, X):
            rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
            population_per_household = X[:, population_ix] / X[:, households_ix]
            if self.add_bedrooms_per_room:
                bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
                return np.c_[X, rooms_per_household, population_per_household,
                             bedrooms_per_room]
            else:
                return np.c_[X, rooms_per_household, population_per_household]
    
    attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
    housing_extra_attribs = attr_adder.transform(housing.to_numpy())
    ```
    

### 2.5.4 특성 스케일링

- 타깃값에 대한 스케일링 일반적으로 불필요
- min-max 스케일링(정규화로 많이 불림)
    - x-min/max-min (0~1 범위로 값을 조정)
    - MinMaxScaler변환기 제공
    - feature_range 매개변수로 범위 변경 가능
- 표준화
    - x-average/std (평균:0, 분산:1)
    - 범위의 상한과 하한이 없어 문제가 될 수 있다. ex) 인공신경망은 입력값의 기대값이 0~1사이
    - 이상치에 영향을 덜 받음
    - StandardScaler변환기 제공
- 훈련데이터에 대해서만 fit()한 후, 훈련 데이터 와 테스트 데이터에 대해 transform() 메서드 사용

### 2.5.5 변환 파이프라인

- Pipeline
    - 연속된 단계를 나타내는 이름/추정기 쌍을 입력받음
    - 마지막 단계는 변환기와 추정기 모두 사용 가능 (나머지는 변환기) → 즉, (fit+transform) or fit_transform 가져야함
        
        ```python
        from sklearn.pipeline import Pipeline
        from sklearn.preprocessing import StandardScaler
        
        num_pipeline = Pipeline([
                ('imputer', SimpleImputer(strategy="median")),
                ('attribs_adder', CombinedAttributesAdder()),
                ('std_scaler', StandardScaler()),
            ])
        
        housing_num_tr = num_pipeline.fit_transform(housing_num)
        ```
        
- ColumnTransformer
    - 하나의 변환기로 각 열마다 적절한 변환을 적용하여 모든 열을 처리
    - 단계
        1. 수치형, 범주형 열 이름의 리스트 생성
        2. ColumnTransformer 객체에 이름, 변환기, 변환기가 적용될 열 이름(또는 인덱스) 튜플의 리스트 입력
        3. 각 변환기를 적절한 열에 적용(반환하는 행의 개수 같아야함)
        4. OneHotEncoder는 희소 행렬, num_pipeline은 밀집 행렬 반환 → 최종 행렬 밀집도 임계값 기준으로 결과 반환(기본적으로 sparse_threshold=0.3)
        5. 여기서는 밀집 행렬 반환
    - Tip
        - “drop”: 삭제하는 열, “passthrough”: 변환을 적용하지 않을 열
        - 나열되지 않은 열을 기본적으로 삭제

        ```python
        from sklearn.compose import ColumnTransformer

        num_attribs = list(housing_num)
        cat_attribs = ["ocean_proximity"]

        full_pipeline = ColumnTransformer([
                ("num", num_pipeline, num_attribs),
                ("cat", OneHotEncoder(), cat_attribs),
            ])

        housing_prepared = full_pipeline.fit_transform(housing)
        ```

## 2.6 모델 선택과 훈련

### 2.6.1 훈련 세트에서 훈련하고 평가하기

- 선형 회귀 모델 → 과소적합
    - 더 복잡한 모델, 더 좋은 특성 주입
    - 더 복잡한 모델 선택 → DecisionTreeRegressor
- 결정 트리 → 과대적합
    - 훈련 세트의 일부분으로 훈련하고 다른 일부분은 검증으로 사용해야함

### 2.6.2 교차 검증을 사용한 평가

- k-fold cross-validation
    - 훈련 세트를 k개의 폴드(fold)로 분할하여 매번 다른 폴드를 평가에 사용 (k번  훈련하고 평가)
- 사이킷런 교차검증 기능 scoring매개변수에 효용함수 기대(클수록 좋은) → 그래서 neg_mean_squared_error, -scores 사용
    
    ```python
    from sklearn.model_selection import cross_val_score
    
    scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                             scoring="neg_mean_squared_error", cv=10)
    tree_rmse_scores = np.sqrt(-scores)
    ```
    
- 앙상블 학습: 여러 다른 모델을 모아서 하나의 모델을 만드는 것
- Tip
    - 교차 검증 점수와 실제 예측값은 물론 하이퍼파라미터와 훈련된 모델 파라미터 모두 저장해야함
    - 파이썬의 pickle, joblib를 사용하여 사이킷런 모델을 간단하게 저장 가능
        
        ```python
        import joblib
        
        joblib.dump(my_model, "my_model.pkl")
        #나중에...
        my_model_loaded=joblib.load("my_model.pkl")
        ```